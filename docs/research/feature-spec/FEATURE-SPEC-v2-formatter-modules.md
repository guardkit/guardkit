# `/feature-spec` v2 Planning: Formatter Modules (Tasks 1–3)
**Date deferred:** 2026-02-22  
**Deferred from:** v1 implementation (FEAT-XXX)  
**Rationale:** Implementation footprint reduction — get working solution first, add infrastructure when real-world usage reveals the need  
**Status:** Preserved planning — ready to spec when v1 ships

---

## Why These Were Deferred

The original spec proposed 5 Python formatter modules (Tasks 1–3) that would:
1. Validate Gherkin syntax output from Claude
2. Detect target project's technology stack
3. Generate stack-specific test scaffolding
4. Generate YAML assumptions manifests with a class
5. Generate feature summary markdown with a class

After analysis, v1 takes a simpler approach: Claude Code's own generation is trusted, files are written inline by the Task 5 orchestration module, and integration tests validate the output. The formatter modules become valuable in v2 when:

- Evidence from real `/feature-spec` runs shows Claude (especially local models on the Dell) generates malformed Gherkin that breaks downstream steps
- The stack detection logic grows beyond a 3-condition `if` block (new stacks added, monorepo support needed)
- The scaffolding templates need to be maintained, versioned, and extended independently from the command logic
- Assumptions need programmatic validation before seeding to Graphiti (e.g., checking that confidence values are valid, IDs are unique)

**Do not implement v2 modules until v1 usage proves they're needed.** This document preserves the design so the knowledge isn't lost.

---

## Task 1 (v2): Gherkin Formatter and Validator Module

### Purpose

A Python module that can parse, validate, and extract structured data from Gherkin content generated by `/feature-spec`. Primarily valuable when running AutoBuild on the Dell ProMax with local models, where generation quality may be lower than Claude Sonnet.

### File location (corrected from v1 spec)

```
guardkit/formatters/__init__.py
guardkit/formatters/gherkin.py
tests/test_gherkin_formatter.py
```

### Design

```python
from dataclasses import dataclass
from typing import Optional

@dataclass
class Step:
    keyword: str           # Given, When, Then, And, But
    text: str              # Step text with parameter placeholders
    parameters: list[str]  # Extracted <parameter> values (for Scenario Outline)

@dataclass  
class Scenario:
    name: str
    steps: list[Step]
    tags: list[str]        # @key-example, @boundary, @negative, @edge-case, @smoke, @regression
    category: str          # Derived from tags: key/boundary/negative/edge-case
    assumptions: list[str] # [ASSUMPTION: ...] comments associated with this scenario
    is_outline: bool
    examples: list[dict]   # Scenario Outline examples table

@dataclass
class ValidationResult:
    valid: bool
    errors: list[str]      # Must-fix issues
    warnings: list[str]    # Should-fix issues

class GherkinFormatter:
    def validate(self, content: str) -> ValidationResult: ...
    def format(self, content: str) -> str: ...
    def extract_scenarios(self, content: str) -> list[Scenario]: ...
```

### Validation rules to implement

The validator should catch common LLM generation errors:
- Missing `Feature:` keyword at file start
- Scenarios with no `When` or `Then` step
- Duplicate scenario names within the same file
- `Background:` appearing after scenarios (must be first)
- `Scenario Outline:` without an `Examples:` table
- Mismatched columns in `Examples` tables (header count ≠ data row count)
- `Given`/`When`/`Then` steps in wrong order (Given after When, etc.)
- Steps without a keyword (bare text lines)
- `[ASSUMPTION]` comments that don't associate with a specific scenario

### Assumption extraction

The validator must extract `# [ASSUMPTION: confidence=X]` comments and associate them with the immediately following scenario. This is the bridge between Gherkin source and the assumptions manifest.

```gherkin
# [ASSUMPTION: confidence=medium] Maximum file size is 50MB
@boundary
Scenario: Reject file exceeding maximum size
    When I upload a file of 50 megabytes plus one byte
    Then the upload should be rejected
```

Extraction: associate the assumption text with the "Reject file exceeding maximum size" scenario.

### Implementation notes

- Use regex-based parsing — do NOT add a Gherkin parser library dependency (behave, gherkin-official, etc.). The goal is catching LLM output errors, not building a Gherkin compiler.
- Parser should be tolerant of Windows line endings (`\r\n`)
- Tags are case-insensitive for matching but preserved as-is in output
- `Scenario Outline` and `Scenario Template` are equivalent keywords

### Test cases to write

```python
def test_valid_gherkin_passes(): ...
def test_missing_feature_keyword(): ...
def test_scenario_without_then(): ...
def test_duplicate_scenario_names(): ...
def test_background_after_scenario(): ...
def test_scenario_outline_without_examples(): ...
def test_examples_table_column_mismatch(): ...
def test_assumption_extraction(): ...
def test_assumption_association_with_scenario(): ...
def test_category_tag_extraction(): ...
def test_windows_line_endings(): ...
```

### When v2 is worthwhile

Build this when: running `/feature-spec` with Qwen3-Coder-30B or other local models and noticing malformed Gherkin appearing in the output (missing Feature keyword, steps out of order, etc.). The integration test in v1 will reveal this pattern.

---

## Task 2 (v2): Stack Detector and Scaffolding Generator

### Purpose

A proper `StackDetector` class with confidence scoring and a `ScaffoldingGenerator` that produces stack-specific BDD step definition skeletons from parsed Gherkin scenarios.

### File locations (corrected from v1 spec)

```
guardkit/formatters/stack_detector.py
guardkit/formatters/scaffolding.py
guardkit/formatters/templates/__init__.py   # NOT guardkit/templates/bdd/ — avoids collision with existing Jinja2 templates
guardkit/formatters/templates/python.py
guardkit/formatters/templates/typescript.py
guardkit/formatters/templates/go.py
guardkit/formatters/templates/generic.py
tests/test_stack_detector.py
tests/test_scaffolding.py
```

**Important:** Templates go in `guardkit/formatters/templates/` — NOT `guardkit/templates/bdd/`. The existing `guardkit/templates/` directory contains Jinja2 `.j2` template files for system context generation. Adding Python modules there would create a confusing mixed-purpose directory. Keep the BDD templates alongside the formatter code that uses them.

### StackDetector design

```python
from dataclasses import dataclass
from pathlib import Path

@dataclass
class StackInfo:
    stack: str              # python, typescript, go, rust, csharp, generic
    confidence: float       # 0.0–1.0
    signals: list[str]      # which files triggered detection
    bdd_runner: str | None  # pytest-bdd, cucumber-js, godog, etc.
    step_extension: str | None  # _steps.py, _steps.ts, _steps_test.go

class StackDetector:
    # Detection priority order — earlier wins
    DETECTION_RULES = [
        ("pyproject.toml", "python", 0.95, "pytest-bdd", "_steps.py"),
        ("requirements.txt", "python", 0.80, "pytest-bdd", "_steps.py"),
        ("setup.py", "python", 0.80, "pytest-bdd", "_steps.py"),
        ("go.mod", "go", 0.95, "godog", "_steps_test.go"),
        ("Cargo.toml", "rust", 0.90, "cucumber-rs", "_steps.rs"),
        # TypeScript detection requires absence of Python signals
        ("tsconfig.json", "typescript", 0.75, "cucumber-js", "_steps.ts"),
        ("package.json", "typescript", 0.60, "cucumber-js", "_steps.ts"),
    ]
    
    def detect(self, project_root: Path) -> StackInfo: ...
```

Polyglot handling (like GuardKit itself): if both `pyproject.toml` and `package.json` are found, the higher-confidence Python detection wins. Log: "Detected Python (pyproject.toml) alongside TypeScript indicators (package.json) — using Python as primary stack. Override with --stack if needed."

### ScaffoldingGenerator design

```python
class ScaffoldingGenerator:
    def generate(
        self, 
        scenarios: list[Scenario],  # From GherkinFormatter.extract_scenarios()
        stack: StackInfo,
        feature_filename: str,
    ) -> dict[str, str]:
        # Returns {filename: content} mapping
        # e.g., {"steps.py": "import pytest\n...", "conftest.py": "..."}
```

Key generation rules:
- Deduplicate steps: if the same step text appears in multiple scenarios, generate the step function once
- For Python: use `parsers.parse()` for parameterised steps (not f-strings or format strings)
- For TypeScript: use Cucumber expression syntax (`{string}`, `{int}`)
- For Go: use regex capture groups
- Generic: return `{}` (no scaffolding)
- Each step body raises `NotImplementedError` (Python), `throw new Error('Not implemented')` (TS), `return godog.ErrPending` (Go)

### Template architecture

Each stack gets a Python module (not a Jinja2 template, because the logic for deduplicating steps and building the scaffold is more complex than simple string interpolation):

```python
# guardkit/formatters/templates/python.py

HEADER = '''"""Step definitions for {feature_name}.
Generated by /feature-spec. Implement the step bodies.
"""
import pytest
from pytest_bdd import scenarios, given, when, then, parsers

scenarios("../{feature_file}")
'''

def generate_step(step: Step) -> str:
    decorator = step.keyword.lower().rstrip(',')
    if step.parameters:
        return f'@{decorator}(parsers.parse("{step.text}"))\ndef {step_fn_name(step)}(...):\n    raise NotImplementedError\n'
    return f'@{decorator}("{step.text}")\ndef {step_fn_name(step)}(...):\n    raise NotImplementedError\n'
```

### When v2 is worthwhile

Build this when: the v1 inline stack detection (3-condition if block) needs to handle more stacks, or when James or another user wants `/feature-spec` for a TypeScript or Go project and the Gherkin-only output isn't sufficient.

---

## Task 3 (v2): Assumptions Manifest and Feature Summary Generators — as Classes

### Purpose

Promote the inline YAML and markdown generation from v1's `feature_spec.py` into proper classes with validation, when the inline approach proves insufficient.

### File locations

```
guardkit/formatters/assumptions.py
guardkit/formatters/feature_summary.py
tests/test_assumptions.py
tests/test_feature_summary.py
```

### AssumptionsGenerator design

```python
class AssumptionsGenerator:
    def generate(
        self,
        scenarios: list[Scenario],    # From GherkinFormatter.extract_scenarios()
        source_description: str,
        feature_name: str,
        date: str,
    ) -> str:
        # Returns YAML string matching the schema in section 4.2 of the v2 spec
        
    def validate(self, yaml_content: str) -> ValidationResult:
        # Checks: required fields present, confidence values are valid (high/medium/low),
        # IDs are unique, human_response values are valid (accept/reject/modify/defer),
        # summary counts match actual assumption counts
```

Validation rules:
- Every assumption must have: `id`, `scenario`, `assumption`, `confidence` (one of high/medium/low), `basis`
- `human_response` is optional at generation time; required before seeding to Graphiti
- `summary.total` must equal `high_confidence + medium_confidence + low_confidence`
- Duplicate `id` values are an error
- Feature with > 5 low-confidence assumptions triggers a warning: "Feature is underspecified"

### FeatureSummaryGenerator design

```python
class FeatureSummaryGenerator:
    def generate(
        self,
        feature_content: str,        # Raw Gherkin content
        source_description: str,
        stack: StackInfo,
        assumptions_yaml: str,
        components: list[dict] | None = None,
    ) -> str:
        # Returns markdown string for /feature-plan consumption
```

The summary must include the stack-appropriate test execution command:
```python
TEST_COMMANDS = {
    "python": "pytest tests/features/{name}_steps.py -v",
    "typescript": "npx cucumber-js features/{name}.feature",
    "go": "go test -v ./features/...",
    "generic": "# No test runner detected — specify with --stack",
}
```

### When v2 is worthwhile

Build when: inline generation in `feature_spec.py` becomes complex enough that it needs independent testing, or when the assumptions manifest needs richer validation before Graphiti seeding (e.g., checking for assumption conflicts with previously seeded assumptions in the knowledge graph).

---

## Trigger Conditions for Starting v2

Start v2 implementation when ANY of the following is true:

1. **Local model quality gate:** Running `/feature-spec` on the Dell with Qwen3-Coder produces `.feature` files that fail integration test parsing more than 20% of the time
2. **Multi-stack usage:** A real project (not GuardKit itself) needs TypeScript or Go scaffolding and the Gherkin-only output creates friction
3. **Assumptions volume:** A single `/feature-spec` run generates more than 10 assumptions, making inline YAML generation hard to maintain
4. **Coach validation gap:** The Coach is failing to catch assumption divergences because Gherkin isn't in Graphiti as structured episodes (this would also trigger the Graphiti seeding work in section 6 of the main analysis)
5. **FeaturePlanContextBuilder integration:** `/feature-plan --from-spec` becomes a real workflow and needs `GherkinFormatter.extract_scenarios()` to map scenarios to task acceptance criteria programmatically

Until these triggers are met, v1's simpler inline approach is the right call.
