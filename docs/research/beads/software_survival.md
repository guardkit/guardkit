Extract from: https://steve-yegge.medium.com/software-survival-3-0-97a2a6255f7b 

Lever 3: Broad Utility
This is the Usage term in the Survival Ratio. It basically amortizes your awareness cost and lowers the threshold for token savings. If you have a truly general-purpose token-saving tool, then it doesn’t really matter if it’s easy for AIs to recreate it. They’ll use the thing that’s everywhere. But how do you make your software be the “obvious” choice for agents?
It’s easy to point at Git and say, “Just be like Git. Be around forever, make sure everyone uses you for decades, and solve a much wider variety of problems than you originally set out to solve.” Same could be said of grep, really. It’s a bit silly; they are too high of a bar. I think a more useful practical example is Temporal, which, despite not being super well-known, is near-universally useful as agentic workflows take center focus in 2026.
Temporal has comparatively high awareness and friction costs, e.g. compared to (say) Postgres, which has been around a lot longer and has much more training data available. But Temporal is as broadly useful as PostgreSQL; just as Postgres can be used to store and query most datasets people care about, Temporal can be used to model and execute most workflows people care about. Temporal has all three levers so far: aggressive insight compression, masterful use of the compute substrate to solve complex problems, and it’s broadly useful. So no AI in its right mind is going to try to clone it for any serious work.
Dolt is another interesting example of software that’s ahead of its time. Gene Kim and I have been saying, “don’t use LLMs for production database access–only use agents in prod when you have Git as a backstop!” Well, what if your database was versioned with Git? Every single change?
Dolt is OSS that has been around for 8 years, and is only now finally finding its killer app with agent-based prod and devops workflows. With Dolt, agents can make mistakes in prod, and roll back (or forward) with the full power of Git. But they hadn’t solved the awareness problem when I first made Beads, or I’d have used Dolt from the start.
You can find problem spaces that will thrive by looking at what will change when agents are doing all the work. For instance, code search gets harder, as LLMs start producing 10x-100x as much code. Agents will need good search as much as humans ever did, and grep, for all its charms, does not scale. So code search engines also have all of our first three levers: (1) they solve a nontrivial problem with lots of hard-to-discover edge cases, (2) they do it in a cheaper computation substrate than GPU inference, and (3) they have found a large, near-universal niche of “anything too big for grep.”
I find all this quite hopeful. There will be infrastructure opportunities galore. The Software 3.0 world is going to be filled with swarms of agents and meta-agents crawling over huge graphs of data, mining interesting insights. There will be a new attention economy, new aggregators to help us know what’s cool, and new channels for broadcasting what your software is capable of.
The more broadly useful you can make your software, the more agents will be able to use it. This can create a virtuous cycle by producing more training data. The new world of software will be big. Aim to build software that lots of agents prefer to use in lots of situations.