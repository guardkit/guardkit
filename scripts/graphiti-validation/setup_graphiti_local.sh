#!/bin/bash
#
# Graphiti MCP Setup Script - Local Configuration
# 
# Sets up Graphiti with Ollama (local LLM) and Sentence Transformers (local embeddings)
# NO external API keys required!
#
# Prerequisites:
# - Docker and Docker Compose installed
# - Ollama installed (brew install ollama)
# - Sufficient RAM for your chosen model
#
# Usage:
#   ./setup_graphiti_local.sh              # Interactive
#   MODEL=qwen2.5:32b-instruct ./setup_graphiti_local.sh  # Specify model
#

set -e

# Configuration
GRAPHITI_DIR="${GRAPHITI_DIR:-$HOME/graphiti-mcp}"
COMPOSE_URL="https://raw.githubusercontent.com/getzep/graphiti/main/mcp_server/docker/docker-compose.yml"

# Default model based on available RAM
get_recommended_model() {
    if [[ "$OSTYPE" == "darwin"* ]]; then
        # macOS - check total RAM
        total_ram_gb=$(sysctl -n hw.memsize | awk '{print int($1/1024/1024/1024)}')
    else
        # Linux
        total_ram_gb=$(free -g | awk '/^Mem:/{print $2}')
    fi
    
    if [ "$total_ram_gb" -ge 80 ]; then
        echo "qwen2.5:72b-instruct-q4_K_M"
    elif [ "$total_ram_gb" -ge 48 ]; then
        echo "qwen2.5:32b-instruct"
    elif [ "$total_ram_gb" -ge 24 ]; then
        echo "qwen2.5:14b-instruct"
    else
        echo "qwen2.5:7b-instruct"
    fi
}

echo "========================================"
echo "Graphiti MCP Local Setup"
echo "========================================"
echo
echo "This setup uses:"
echo "  - Ollama for LLM (local, no API key)"
echo "  - Sentence Transformers for embeddings (local)"
echo "  - FalkorDB for graph storage (Docker)"
echo

# Check Docker
if ! command -v docker &> /dev/null; then
    echo "❌ Error: Docker is not installed"
    echo "   Install from: https://docker.com"
    exit 1
fi

if ! docker info &> /dev/null 2>&1; then
    echo "❌ Error: Docker is not running"
    echo "   Please start Docker Desktop"
    exit 1
fi
echo "✓ Docker is available"

# Check Ollama
if ! command -v ollama &> /dev/null; then
    echo "❌ Error: Ollama is not installed"
    echo "   Install with: brew install ollama"
    exit 1
fi
echo "✓ Ollama is installed"

# Check if Ollama is running
if ! curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
    echo ""
    echo "⚠️  Ollama is not running. Starting it..."
    ollama serve &
    sleep 3
fi
echo "✓ Ollama is running"

# Determine model
if [ -z "$MODEL" ]; then
    RECOMMENDED_MODEL=$(get_recommended_model)
    echo ""
    echo "Recommended model for your system: $RECOMMENDED_MODEL"
    echo ""
    echo "Press Enter to use this model, or type a different one:"
    read -r user_model
    MODEL="${user_model:-$RECOMMENDED_MODEL}"
fi

echo ""
echo "Using model: $MODEL"

# Check if model exists, pull if not
if ! ollama list | grep -q "$MODEL"; then
    echo ""
    echo "Pulling model $MODEL (this may take a while)..."
    ollama pull "$MODEL"
fi
echo "✓ Model $MODEL is available"

# Create directory
echo ""
echo "Setting up in: $GRAPHITI_DIR"
mkdir -p "$GRAPHITI_DIR"
cd "$GRAPHITI_DIR"

# Download docker-compose.yml
echo "Downloading docker-compose.yml..."
curl -fsSL -o docker-compose.yml "$COMPOSE_URL"
echo "✓ Downloaded docker-compose.yml"

# Create .env file
cat > .env << EOF
# ===========================================
# LOCAL SETUP - No external API dependencies
# ===========================================
# Generated by setup_graphiti_local.sh

# Graphiti Configuration
GRAPHITI_GROUP_ID=guardkit
GRAPHITI_TELEMETRY_ENABLED=false

# Placeholder (not used for local setup)
OPENAI_API_KEY=not-used-local-setup

# FalkorDB
FALKORDB_PASSWORD=
EOF
echo "✓ Created .env file"

# Create config.yaml for local setup
cat > config.yaml << EOF
# ===========================================
# Graphiti MCP - Fully Local Configuration
# ===========================================
# Generated by setup_graphiti_local.sh
# No external API calls - runs entirely on your machine

server:
  transport: "http"
  host: "0.0.0.0"
  port: 8000

# LLM - Use local Ollama
llm:
  provider: "openai"  # Ollama provides OpenAI-compatible API
  model: "$MODEL"
  api_base: "http://host.docker.internal:11434/v1"
  api_key: "ollama"  # Dummy key, Ollama doesn't check

# Embeddings - Use local Sentence Transformers  
embedder:
  provider: "sentence_transformers"
  model: "all-MiniLM-L6-v2"  # Downloads automatically, runs locally

# Database
database:
  provider: "falkordb"
EOF
echo "✓ Created config.yaml for local setup"

# Start containers
echo ""
echo "Starting Docker containers..."
docker compose up -d

echo ""
echo "Waiting for services to be ready..."
sleep 5

# Health check
echo "Checking health..."
max_attempts=12
attempt=1
while [ $attempt -le $max_attempts ]; do
    if curl -sf http://localhost:8000/health > /dev/null 2>&1; then
        echo "✓ MCP server is healthy"
        break
    fi
    echo "   Waiting for MCP server... (attempt $attempt/$max_attempts)"
    sleep 5
    attempt=$((attempt + 1))
done

if [ $attempt -gt $max_attempts ]; then
    echo "⚠️  MCP server not responding yet"
    echo "   Check logs: docker compose logs -f"
fi

echo ""
echo "========================================"
echo "Setup Complete!"
echo "========================================"
echo ""
echo "Configuration:"
echo "  - LLM Model: $MODEL (via Ollama)"
echo "  - Embeddings: all-MiniLM-L6-v2 (Sentence Transformers)"
echo "  - Database: FalkorDB"
echo ""
echo "Services running:"
echo "  - Ollama API:      http://localhost:11434"
echo "  - FalkorDB:        redis://localhost:6379"
echo "  - FalkorDB UI:     http://localhost:3000"
echo "  - MCP Server:      http://localhost:8000/mcp/"
echo "  - Health Check:    http://localhost:8000/health"
echo ""
echo "Test commands:"
echo "  curl http://localhost:8000/health"
echo "  curl http://localhost:11434/api/tags"
echo ""
echo "Run validation tests:"
echo "  pip install httpx"
echo "  python $GRAPHITI_DIR/test_connection.py"
echo ""
echo "View logs:"
echo "  docker compose logs -f"
echo ""
echo "Stop services:"
echo "  cd $GRAPHITI_DIR && docker compose down"
echo "  pkill ollama  # Optional"
echo ""
