# Task Work - Unified Implementation Command

## Feature Detection

This command supports **graceful degradation** based on installed packages:

### GuardKit Only (Core Workflow)
- Loads task description, acceptance criteria, implementation notes
- Executes full workflow with architectural review and quality gates
- No requirements/epic loading (require-kit features)

### GuardKit + Require-Kit (Enhanced Workflow)
- All core features PLUS:
- Loads EARS requirements if linked in task frontmatter
- Loads Gherkin scenarios if linked (for BDD workflow)
- Includes epic/feature context for hierarchy
- Enables requirements-based acceptance criteria enrichment

**Note**: The workflow automatically detects require-kit availability and adjusts Phase 1 loading accordingly. No manual configuration required.

---

## âš ï¸ Working Directory Requirement

**CRITICAL**: `/task-work` must be run from your **project root directory** (where code files should be created).

The command uses the current working directory to:
- Detect project technology stack (e.g., `.csproj` for .NET, `package.json` for Node.js)
- Create source files in the correct locations
- Run tests and build commands
- Generate implementation files

### Verify Your Location

Before running `/task-work`, confirm you're in the correct directory:

```bash
# âœ… Correct - Run from project root
cd ~/Projects/weather_demo
pwd  # Should show: /Users/you/Projects/weather_demo
ls   # Should show: weather_demo.csproj, Program.cs, Controllers/, tasks/, etc.
/task-work TASK-001

# âŒ Wrong - Running from RequireKit/GuardKit directory
cd ~/Projects/require-kit  # Wrong location!
/task-work TASK-001        # Will create files in wrong place, wrong stack detection
```

### What Happens If You're in the Wrong Directory?

If you run `/task-work` from the wrong directory:
- âŒ Files created in wrong location (e.g., in `require-kit/` instead of `weather_demo/`)
- âŒ Technology stack misdetected (sees RequireKit's Python files instead of your .NET project)
- âŒ Wrong test commands executed (runs `pytest` instead of `dotnet test`)
- âŒ Quality gates fail or execute against wrong codebase

### Directory Structure Example

```
~/Projects/
â”œâ”€â”€ require-kit/           # RequireKit installation (don't run task-work here!)
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ ...
â”œâ”€â”€ guardkit/            # GuardKit repo (don't run task-work here!)
â”‚   â””â”€â”€ ...
â””â”€â”€ weather_demo/          # Your project (run task-work HERE âœ…)
    â”œâ”€â”€ weather_demo.csproj
    â”œâ”€â”€ Program.cs
    â”œâ”€â”€ Controllers/
    â””â”€â”€ tasks/
        â””â”€â”€ backlog/TASK-001-*.md
```

### Integration with RequireKit

When using RequireKit for requirements management:
1. Create requirements in RequireKit directory: `/req-create`
2. **Navigate to project directory**: `cd ~/Projects/weather_demo`
3. Create task linked to requirements: `/task-create "Title" requirements:[REQ-001]`
4. Work on task **from project directory**: `/task-work TASK-001`

### Quick Directory Validation

Run this before `/task-work` to verify you're in the right place:

```bash
# Verify you're in project root (should see project files)
ls *.csproj 2>/dev/null || ls package.json 2>/dev/null || ls requirements.txt 2>/dev/null || ls *.sln 2>/dev/null

# If you see your project files, you're good to go!
```

---

## Command Syntax

```bash
/task-work TASK-XXX [--mode=standard|tdd|bdd] [--intensity=minimal|light|standard|strict] [--design-only | --implement-only] [--docs=minimal|standard|comprehensive] [--no-questions | --with-questions | --defaults | --answers="1:Y 2:N 3:JWT"] [other-flags...]
```

## Available Flags

| Flag | Description |
|------|-------------|
| `--mode=tdd\|standard\|bdd` | Development mode (default: standard) |
| `--intensity=LEVEL` | Control ceremony level (minimal, light, standard, strict) |
| `--micro` | Alias for --intensity=minimal |
| `--design-only` | Stop at Phase 2.8 checkpoint, save plan |
| `--implement-only` | Start at Phase 3 with approved plan |
| `--docs=minimal\|standard\|comprehensive` | Documentation level control |
| `--no-questions` | Skip Phase 1.5 clarification |
| `--with-questions` | Force Phase 1.5 clarification |
| `--defaults` | Use clarification defaults without prompting |
| `--answers="..."` | Inline clarification answers for automation |
| `--reclarify` | Re-run clarification (ignore saved decisions) |

## Documentation Level Control (NEW - TASK-036)

Control the verbosity of documentation generated during task execution. This significantly impacts execution time and token consumption.

### Flag: --docs=LEVEL

**Purpose**: Override automatic documentation level selection

**Values**:
- `--docs=minimal` - Structured data only, ~8-12 minutes, 2 files (DEFAULT)
- `--docs=standard` - Brief explanations, ~12-18 minutes, 2 files
- `--docs=comprehensive` - Full documentation, ~36+ minutes, 13+ files

**Auto-selection** (when flag not provided):
- Default: `minimal` mode (use `--docs=standard` to lift)
- Security/compliance keywords: `comprehensive` mode (forced)

**Configuration hierarchy** (highest to lowest priority):
1. Command-line flag (`--docs=minimal|standard|comprehensive`)
2. Force-comprehensive triggers (security, compliance, breaking changes)
3. Settings.json default (`.claude/settings.json` â†’ `documentation.default_level`)
4. Default: `minimal` (use `--docs=standard` to lift)

**Examples**:
```bash
# Default minimal mode (fastest, recommended)
/task-work TASK-042

# Lift to standard mode when more documentation needed
/task-work TASK-043 --docs=standard

# Explicit comprehensive mode (security tasks)
/task-work TASK-044 --docs=comprehensive
```

**Performance impact**:
| Level | Duration | Files | Tokens | Use When |
|-------|----------|-------|--------|----------|
| **minimal** | 8-12 min | 2 | 100-150k | Simple tasks, fast iteration |
| **standard** | 12-18 min | 2 | 150-250k | Normal development |
| **comprehensive** | 36+ min | 13+ | 500k+ | Security, compliance, complex tasks |

**Agent context format**:
All agents receive documentation level via `<AGENT_CONTEXT>` block in prompts:
```
<AGENT_CONTEXT>
documentation_level: minimal|standard|comprehensive
complexity_score: {1-10}
task_id: TASK-XXX
stack: {detected_stack}
phase: {1|2|2.5|4|5}
</AGENT_CONTEXT>
```

See individual agent files (installer/core/agents/*.md) for documentation level behavior specifications.

## Intensity Levels (NEW - TASK-INT-c3d4)

The `--intensity` flag controls the ceremony level and phase execution profile, allowing you to tune the workflow for task complexity and team preference.

### Flag: --intensity=LEVEL

**Purpose**: Select predefined phase execution profiles from a spectrum of ceremony levels.

**Values**:
- `minimal` - Fastest execution, minimal phases (alias: `--micro`)
- `light` - Fast execution with brief planning, no architecture review
- `standard` - Full workflow with smart MCP usage (default, current behavior)
- `strict` - Maximum rigor, all phases with blocking checkpoints

**Default**: `standard` (provides current behavior)

### Intensity Level Specifications

#### minimal (--micro alias)

**Use for**: Trivial tasks, cosmetic changes, typo fixes, simple documentation updates.

**Phases Executed**:
- Phase 1: Load context âœ“
- Phase 2: Planning âœ—
- Phase 2.5A: Pattern MCP âœ—
- Phase 2.5B: Architectural Review âœ—
- Phase 2.7: Complexity Evaluation âœ—
- Phase 2.8: Human Checkpoint âœ—
- Phase 3: Implementation âœ“ (simplified)
- Phase 4: Testing âœ“ (no coverage requirement)
- Phase 4.5: Fix Loop âœ“ (1 attempt max)
- Phase 5: Code Review âœ“ (lint only)
- Phase 5.5: Plan Audit âœ—

**Key Characteristics**:
- Execution time: 3-5 minutes
- No implementation plan generated
- Coverage requirements skipped
- Minimal human interaction
- Quick architectural validation (lint only)

**Quality Gates**:
- Compilation: REQUIRED
- Tests Pass: REQUIRED
- Coverage: SKIPPED
- Architectural Review: SKIPPED
- Code Review: Lightweight (lint only)

**Example**:
```bash
# Fix a typo in error message
/task-work TASK-047 --intensity=minimal

# Or use the --micro alias
/task-work TASK-047 --micro
```

#### light

**Use for**: Simple features, straightforward bug fixes, small refactoring tasks.

**Phases Executed**:
- Phase 1: Load context âœ“
- Phase 2: Planning âœ“ (brief, ~5 minutes)
- Phase 2.5A: Pattern MCP âœ—
- Phase 2.5B: Architectural Review âœ—
- Phase 2.7: Complexity Evaluation âœ—
- Phase 2.8: Human Checkpoint âœ“ (10s timeout, auto-proceed)
- Phase 3: Implementation âœ“
- Phase 4: Testing âœ“
- Phase 4.5: Fix Loop âœ“ (2 attempts)
- Phase 5: Code Review âœ“ (quick)
- Phase 5.5: Plan Audit âœ“ (50% variance threshold)

**Key Characteristics**:
- Execution time: 10-15 minutes
- Brief implementation plan (essential elements only)
- Optional checkpoint with auto-proceed
- Faster planning process
- Lighter scope creep detection

**Quality Gates**:
- Compilation: REQUIRED
- Tests Pass: REQUIRED
- Coverage: â‰¥70% (vs 80% in standard)
- Architectural Review: SKIPPED
- Code Review: Quick pass (no detailed analysis)

**Plan Audit Variance Thresholds**:
- LOC variance: Â±50% (vs Â±20% in standard)
- Duration variance: Â±50% (vs Â±30% in standard)

**Example**:
```bash
# Add a simple feature with quick review
/task-work TASK-048 --intensity=light
```

#### standard (default)

**Use for**: Most tasks, normal development, features with clear requirements.

**This is the current default behavior. All phases execute with smart decisions**:
- Phase 1: Load context âœ“
- Phase 2: Planning âœ“ (full)
- Phase 2.5A: Pattern MCP âœ“ (only if pattern need detected)
- Phase 2.5B: Architectural Review âœ“
- Phase 2.7: Complexity Evaluation âœ“
- Phase 2.8: Human Checkpoint âœ“ (30s timeout, auto-proceed for 1-6, blocking for 7-10)
- Phase 3: Implementation âœ“
- Phase 4: Testing âœ“
- Phase 4.5: Fix Loop âœ“ (3 attempts)
- Phase 5: Code Review âœ“ (full)
- Phase 5.5: Plan Audit âœ“ (20% variance threshold)

**Key Characteristics**:
- Execution time: 15-30 minutes
- Complete implementation plan
- Full architectural review when beneficial
- Complexity-gated checkpoints
- Standard scope creep detection

**Quality Gates**:
- Compilation: REQUIRED
- Tests Pass: REQUIRED
- Coverage: â‰¥80% lines, â‰¥75% branches
- Architectural Review: â‰¥60/100 (human checkpoint if lower)
- Code Review: Full analysis
- Pattern Review: Smart MCP usage

**Plan Audit Variance Thresholds**:
- LOC variance: Â±20% acceptable
- Duration variance: Â±30% acceptable
- File count variance: 0% (must match plan exactly)

**Example**:
```bash
# Standard workflow - full quality gates
/task-work TASK-049  # Same as /task-work TASK-049 --intensity=standard
```

#### strict

**Use for**: Critical code, security-sensitive changes, APIs, high-risk refactoring, financial systems.

**Phases Executed** (maximum rigor):
- Phase 1: Load context âœ“
- Phase 2: Planning âœ“ (detailed)
- Phase 2.5A: Pattern MCP âœ“ (always, comprehensive pattern analysis)
- Phase 2.5B: Architectural Review âœ“ (full with security scan)
- Phase 2.7: Complexity Evaluation âœ“ (detailed)
- Phase 2.8: Human Checkpoint âœ“ (blocking, no timeout)
- Phase 3: Implementation âœ“
- Phase 4: Testing âœ“ (comprehensive)
- Phase 4.5: Fix Loop âœ“ (5 attempts)
- Phase 5: Code Review âœ“ (full + security scan)
- Phase 5.5: Plan Audit âœ“ (0% variance - any deviation flagged)

**Key Characteristics**:
- Execution time: 30-60+ minutes
- Comprehensive implementation plan
- Mandatory human checkpoint
- Full pattern analysis
- Security vulnerability scanning
- Zero-tolerance scope creep

**Quality Gates**:
- Compilation: REQUIRED
- Tests Pass: REQUIRED (all must pass)
- Coverage: â‰¥85% lines, â‰¥80% branches (elevated requirements)
- Architectural Review: â‰¥70/100 minimum, human checkpoint if lower
- Code Review: Full analysis + security scan
- Pattern Review: Comprehensive pattern analysis

**Plan Audit Variance Thresholds**:
- LOC variance: 0% variance allowed (any deviation flagged for review)
- Duration variance: Â±10% only
- File count variance: 0% (exact match required)

**Blocking Checkpoints**:
- Phase 2.8: Mandatory, no timeout
- Phase 5: Full security review before approval

**Example**:
```bash
# Security-critical implementation with maximum rigor
/task-work TASK-050 --intensity=strict

# Financial system changes
/task-work TASK-051 --intensity=strict

# API endpoint changes
/task-work TASK-052 --intensity=strict
```

### Intensity Selection Guide

| Task Type | Recommended | Reason |
|-----------|-------------|--------|
| Typo fix | minimal | Skip unnecessary phases |
| Documentation update | minimal | Documentation-only exception |
| Simple bug fix | light | Brief planning, quick review |
| New UI component | standard | Full architecture review beneficial |
| Business logic feature | standard | Standard rigor recommended |
| Security implementation | strict | Mandatory security review |
| API endpoint changes | strict | Breaking changes require strict mode |
| Database migration | strict | High risk, zero-tolerance scope creep |
| Authentication changes | strict | Security-critical |

### Flag Combinations

**Valid combinations**:
```bash
# Intensity + mode
/task-work TASK-001 --intensity=strict --mode=tdd

# Intensity + documentation
/task-work TASK-002 --intensity=light --docs=comprehensive

# Intensity + clarification (--no-questions for automation)
/task-work TASK-003 --intensity=minimal --no-questions

# But NOT intensity + design flags (conflict)
# âŒ /task-work TASK-001 --intensity=strict --design-only  # Invalid
```

**Note**: `--intensity` cannot be combined with `--design-only` or `--implement-only`. Use design flags for the default intensity workflow only.

### Intensity Auto-Detection (NEW - TASK-INT-e5f6)

When `--intensity` is not explicitly provided, the system automatically detects the appropriate intensity level based on **provenance** and **complexity**.

#### Detection Algorithm

The auto-detection follows a prioritized decision tree:

1. **High-Risk Keywords** â†’ STRICT (always overrides)
2. **Provenance: parent_review** â†’ MINIMAL/LIGHT based on complexity
3. **Provenance: feature_id** â†’ MINIMAL/LIGHT/STANDARD based on complexity
4. **Fresh task** â†’ Complexity-based detection

#### Provenance-Based Rules

**Tasks from Reviews** (have `parent_review` field):
- Complexity â‰¤4 â†’ **MINIMAL**
- Complexity >4 â†’ **LIGHT**

**Rationale**: Tasks created from review recommendations are already well-scoped by the review process, so they can safely use lighter intensity levels.

**Tasks from Features** (have `feature_id` field):
- Complexity â‰¤3 â†’ **MINIMAL**
- Complexity â‰¤5 â†’ **LIGHT**
- Complexity >5 â†’ **STANDARD**

**Rationale**: Feature subtasks benefit from feature-level planning and coordination, reducing the need for individual task rigor.

**Fresh Tasks** (no provenance):
- Complexity â‰¤3 â†’ **MINIMAL**
- Complexity â‰¤5 â†’ **LIGHT**
- Complexity â‰¤6 â†’ **STANDARD**
- Complexity >6 â†’ **STRICT**

**Rationale**: Fresh tasks require more rigor since they lack the benefit of prior planning from reviews or features.

#### High-Risk Keywords

The following keywords in the task description **always force STRICT mode**, regardless of complexity or provenance:

**Security & Authentication**:
- security, auth, authentication, authorization
- oauth, saml, jwt, session
- privilege, permission, access control
- encryption, crypto, cryptographic

**Data & Schema**:
- schema, migration, database

**Breaking Changes**:
- breaking, breaking change, api, endpoint

**Financial**:
- financial, payment, billing

**Vulnerabilities**:
- injection, xss, csrf

**Rationale**: These keywords indicate security-sensitive or high-risk changes that demand maximum rigor regardless of task complexity.

#### Examples

```bash
# Example 1: Task from review with low complexity
# Task: TASK-042
# Description: "Fix typo in error message"
# Complexity: 2
# parent_review: TASK-041
# â†’ Auto-detected: MINIMAL

# Example 2: Task from feature with medium complexity
# Task: TASK-043
# Description: "Add dark mode toggle component"
# Complexity: 5
# feature_id: dark-mode
# â†’ Auto-detected: LIGHT

# Example 3: Fresh task with high-risk keyword
# Task: TASK-044
# Description: "Implement OAuth authentication"
# Complexity: 6
# parent_review: None
# feature_id: None
# â†’ Auto-detected: STRICT (high-risk keyword: "oauth")

# Example 4: Fresh task with high complexity
# Task: TASK-045
# Description: "Refactor payment processing system"
# Complexity: 8
# parent_review: None
# feature_id: None
# â†’ Auto-detected: STRICT (high complexity + "payment" keyword)

# Example 5: User override takes precedence
# Task: TASK-046
# Auto-detected: LIGHT
# â†’ /task-work TASK-046 --intensity=strict
# â†’ Actual: STRICT (user override)
```

#### Provenance Field Expectations

The auto-detection algorithm expects the following fields in the task metadata:

```yaml
# Task frontmatter
task_id: TASK-042
description: "Add user authentication"
complexity: 6
parent_review: TASK-041  # Optional - set if created from review
feature_id: auth-feature # Optional - set if part of feature
```

**Field Sources**:
- `parent_review`: Set by `/task-review` when creating implementation tasks via [I]mplement decision
- `feature_id`: Set by `/feature-plan` when creating feature subtasks
- `complexity`: Set by complexity evaluation during task creation

#### Implementation Details

The auto-detection logic is implemented in `guardkit/orchestrator/intensity_detector.py`:

```python
from guardkit.orchestrator.intensity_detector import (
    IntensityLevel,
    determine_intensity,
    HIGH_RISK_KEYWORDS,
)

# Auto-detect intensity
task_data = {
    "description": task.description,
    "complexity": task.complexity,
    "parent_review": task.parent_review,
    "feature_id": task.feature_id,
}
intensity = determine_intensity(task_data, override=args.intensity)
```

**Module Characteristics**:
- Pure stateless functions (no side effects)
- Dict-based input (no Pydantic coupling)
- Enum-based type safety
- Graceful handling of missing/invalid data

#### Override Behavior

User-provided `--intensity` flag always takes precedence over auto-detection:

```bash
# Auto-detection would choose LIGHT, but user forces STRICT
/task-work TASK-042 --intensity=strict
```

Invalid override values fall back to auto-detection with a warning:

```bash
# Invalid value
/task-work TASK-042 --intensity=invalid
# Warning: Invalid intensity override 'invalid', falling back to auto-detection
# â†’ Uses auto-detected intensity
```

#### Logging

Auto-detection decisions are logged for transparency:

```
INFO: Task from review (parent_review=TASK-041), complexity=3 â†’ minimal
INFO: High-risk keywords detected in description, forcing STRICT intensity
INFO: Task from feature (feature_id=auth-feature), complexity=5 â†’ light
INFO: Fresh task with complexity=7 â†’ strict
```

## Micro-Task Mode (NEW - TASK-020)

The task-work command now supports a `--micro` flag for streamlined execution of trivial tasks (typo fixes, documentation updates, cosmetic changes) that don't require full architectural review.

### Flag: --micro

**Purpose**: Lightweight workflow for trivial tasks, completing in 3-5 minutes vs 15+ minutes.

**Criteria for micro-tasks** (ALL must be true) - TASK-TWP-c3d4 updated thresholds:
- Complexity: â‰¤3/10 (was 1/10 - simple tasks now qualify)
- Files: â‰¤3 file modifications (was single file)
- Risk: No high-risk keywords (security, schema, breaking changes, API changes)
- Estimated time: <2 hours (was <1 hour)

**Phases executed**:
- Phase 1: Load Task Context
- Phase 3: Implementation (simplified)
- Phase 4: Quick Testing (compilation + tests only, no coverage)
- Phase 4.5: Fix Loop (1 attempt max, vs 3 in standard)
- Phase 5: Quick Review (lint only, skip SOLID/DRY/YAGNI)

**Phases skipped**:
- Phase 2: Implementation Planning
- Phase 2.5A: Pattern Suggestion
- Phase 2.5B: Architectural Review
- Phase 2.6: Human Checkpoint
- Phase 2.7: Complexity Evaluation
- Phase 5.5: Plan Audit (no plan in micro-task mode)

**Quality gates** (micro-task specific):
- Compilation: REQUIRED (same as standard)
- Tests Pass: REQUIRED (same as standard)
- Coverage: SKIPPED (not required for micro-tasks)
- Architectural Review: SKIPPED
- Code Review: LIGHTWEIGHT (lint only)

**Auto-detection**: System automatically suggests `--micro` flag when task qualifies:
- Analyzes task metadata (title, description, estimated effort)
- Detects high-risk keywords (security, database, API, etc.)
- Shows suggestion with 10-second timeout
- User can accept suggestion or continue with full workflow

**Validation**: If `--micro` flag is used but task doesn't qualify, escalates to full workflow with warning.

**Example** (micro-task success):
```bash
/task-work TASK-047 --micro

Micro-Task Mode Enabled

Phase 1: Load Task Context
  Task: Fix typo in error message
  File: src/services/AuthService.py
  Change: 'occured' â†’ 'occurred'

Phase 3: Implementation
  Updated src/services/AuthService.py:45
  Changed error message

Phase 4: Quick Testing
  Compilation: PASSED
  Tests: 5/5 PASSED (coverage skipped)

Phase 5: Quick Review
  Lint: PASSED (no issues)

Task State: BACKLOG â†’ IN_REVIEW
Duration: 2 minutes 34 seconds
```

**Example** (auto-detection):
```bash
/task-work TASK-047

Detected micro-task (confidence: 95%)
This task appears to be trivial (complexity â‰¤3/10, â‰¤3 files, <2 hours).

Suggest using: /task-work TASK-047 --micro
Saves ~12 minutes by skipping optional phases.

Auto-apply micro-mode? [y/N] (10s timeout): _
```

**Example** (escalation):
```bash
/task-work TASK-048 --micro

Task does not qualify as micro-task:
  - Complexity: 5/10 (threshold: â‰¤3/10)
  - High-risk keywords detected: authentication, database
  - Estimated effort: 4 hours (threshold: <2 hours)

Escalating to full workflow...

Phase 1: Load Task Context
Phase 2: Implementation Planning
Phase 2.5B: Architectural Review
...
(continues with full workflow)
```

**Documentation-only exception**: Tasks affecting only documentation files (.md, .txt, .rst) automatically qualify for micro-task mode, even if they affect multiple files.

**Use cases**:
- Typo fixes in code or documentation
- Comment updates and documentation improvements
- Cosmetic changes (formatting, whitespace)
- Simple configuration changes
- Minor UI text updates
- Small refactoring (variable renaming, etc.)

**NOT for**:
- Security-related changes (authentication, authorization, encryption)
- Database schema changes (migrations, table alterations)
- API changes (breaking changes, new endpoints)
- External integrations (third-party APIs, webhooks)
- Multi-file refactoring
- Complex business logic changes

## Design-First Workflow Flags (TASK-006)

The task-work command now supports optional flags for design-first workflow, enabling flexible execution modes based on task complexity and team collaboration needs.

### Flag: --design-only

**Purpose**: Execute design phases only, stop at approval checkpoint.

**Phases executed**:
- Phase 1: Load Task Context
- Phase 2: Implementation Planning
- Phase 2.5A: Pattern Suggestion (if Design Patterns MCP available)
- Phase 2.5B: Architectural Review
- Phase 2.7: Complexity Evaluation & Plan Persistence
- Phase 2.8: Human Checkpoint (mandatory for design-only)

**Phases skipped**:
- Phase 3: Implementation
- Phase 4: Testing
- Phase 4.5: Fix Loop
- Phase 5: Code Review

**Outcome**: Task moves to `design_approved` state with saved implementation plan.

**Use cases**:
- Complex tasks (complexity â‰¥ 7) requiring upfront design approval
- Multi-day tasks where design and implementation happen on different days
- Architect-led design with developer-led implementation
- High-risk changes (security, breaking changes, schema changes)
- Unclear requirements needing design exploration

**Example**:
```bash
/task-work TASK-006 --design-only
```

### Flag: --implement-only

**Purpose**: Execute implementation phases using previously approved design.

**Prerequisite**: Task MUST be in `design_approved` state (approved via --design-only).

**Phases executed**:
- Phase 3: Implementation (using saved plan)
- Phase 4: Testing
- Phase 4.5: Fix Loop (ensure tests pass)
- Phase 5: Code Review

**Phases skipped**:
- Phase 1-2.8 (uses saved design from --design-only run)

**Outcome**: Task moves to `in_review` state (if quality gates pass) or `blocked` (if tests fail).

**Use cases**:
- Implementing previously approved designs
- Continuing work after design approval on different day
- Different person implementing than who designed
- Multi-day task workflow (design Day 1, implement Day 2)

**Example**:
```bash
/task-work TASK-006 --implement-only
```

### No Flags (Default Behavior - Unchanged)

**Purpose**: Execute complete workflow in single session.

**Phases executed**: All phases in sequence (1 â†’ 2 â†’ 2.5A â†’ 2.5B â†’ 2.7 â†’ 2.8 â†’ 3 â†’ 4 â†’ 4.5 â†’ 5)

**Phase 2.8 checkpoint**: Triggered based on complexity evaluation (auto-proceed for 1-3, optional for 4-6, mandatory for 7-10).

**Use cases**:
- Simple to medium tasks (complexity 1-6)
- Straightforward implementation with clear approach
- Single developer handling both design and implementation
- Design and implementation can happen in same session
- Low-risk changes

**Example**:
```bash
/task-work TASK-006
```

### Flag Validation Rules

**Mutual Exclusivity**: --design-only and --implement-only cannot be used together.

```bash
# âŒ Invalid usage
/task-work TASK-006 --design-only --implement-only

# Error message:
âŒ Error: Cannot use both --design-only and --implement-only flags together

Choose one workflow mode:
  --design-only     Execute design phases only (Phases 1-2.8)
  --implement-only  Execute implementation phases only (Phases 3-5)
  (no flags)        Execute complete workflow (default)
```

**State Validation**: --implement-only requires task to be in `design_approved` state.

```bash
# âŒ Invalid usage (task not in design_approved state)
/task-work TASK-006 --implement-only

# Error message:
âŒ Cannot execute --implement-only workflow

Task TASK-006 is in 'backlog' state.
Required state: design_approved

To approve design first, run:
  /task-work TASK-006 --design-only

Or run complete workflow without flags:
  /task-work TASK-006
```

### New Task State: design_approved

Tasks can now be in a `design_approved` state:
- **Location**: `tasks/design_approved/{task_id}.md`
- **Purpose**: Indicates design has been approved and is ready for implementation
- **Metadata**: Includes saved implementation plan, architectural review scores, complexity evaluation

### Design Metadata Schema

When using --design-only, the following metadata is saved to task frontmatter:

```yaml
design:
  status: approved  # pending, approved, rejected, n/a
  approved_at: "2025-10-11T14:30:00Z"
  approved_by: "human"  # or "auto" for simple tasks
  implementation_plan_version: "v1"
  architectural_review_score: 85
  complexity_score: 7
  design_session_id: "design-TASK-006-20251011143000"
  design_notes: "Architectural review passed, ready for implementation"
```

### Implementation Plan Storage

Design plans are saved to:
```
docs/state/{task_id}/implementation_plan.json
```

This file contains:
- Files to create/modify
- External dependencies
- Estimated duration and LOC
- Implementation phases
- Test strategy
- Risk mitigations
- Architectural review results

## Clarifying Questions Flags (NEW - TASK-CLQ-007)

The task-work command now supports flags to control Phase 1.6 (Clarifying Questions) behavior, enabling flexible clarification workflows for different task complexities and automation scenarios.

### Flag: --no-questions

**Purpose**: Skip Phase 1.6 (Clarifying Questions) entirely and proceed directly from context loading to implementation planning.

**Use cases**:
- CI/CD automation where human input is not available
- Re-running tasks with previously clarified scope
- Tasks with complete specification in description
- Trivial tasks (complexity 1-2) where clarification adds no value
- Fast iteration during prototyping

**Example**:
```bash
/task-work TASK-a3f8 --no-questions
```

**Behavior**:
- Phase 1.6 is skipped regardless of task complexity
- Implementation planning proceeds with task description as-is
- Any ambiguities are resolved using default assumptions
- No user interaction required during execution

### Flag: --with-questions

**Purpose**: Force Phase 1.6 (Clarifying Questions) even for trivial tasks (complexity 1-2).

**Use cases**:
- Learning mode - understand what clarifications are available
- High-stakes tasks where even trivial scope needs confirmation
- Tasks where default assumptions may be incorrect
- Training new team members on clarification patterns

**Example**:
```bash
/task-work TASK-b2c4 --with-questions
```

**Behavior**:
- Phase 1.6 executes regardless of complexity score
- Questions are presented based on detected ambiguity
- For complexity 1-2: Uses quick mode (15s timeout)
- For complexity 3+: Uses appropriate mode based on score

### Flag: --defaults

**Purpose**: Proceed through Phase 1.6 using all default answers without prompting user.

**Use cases**:
- CI/CD pipelines requiring deterministic behavior
- Batch processing multiple tasks
- Testing workflows without manual intervention
- Quick iteration where defaults are acceptable

**Example**:
```bash
/task-work TASK-c5d7 --defaults
```

**Behavior**:
- Phase 1.6 executes (questions are generated)
- All questions are answered with their default values automatically
- No user interaction required
- Clarification context is still passed to Phase 2
- Useful for understanding what questions would be asked

### Flag: --answers="1:Y 2:N 3:JWT"

**Purpose**: Provide inline answers to clarifying questions for automation.

**Format**: Space-separated question-answer pairs using question number and answer code.

**Use cases**:
- CI/CD pipelines with predetermined answers
- Scripted task execution with known parameters
- Integration testing of clarification workflows
- Batch processing with consistent choices

**Example**:
```bash
# Answer 3 questions inline
/task-work TASK-d4e9 --answers="1:S 2:I 3:JWT"

# Breakdown:
# Q1: Implementation Scope â†’ [S]tandard
# Q2: Testing Approach â†’ [I]ntegration tests
# Q3: Auth Strategy â†’ JWT
```

**Behavior**:
- Phase 1.6 executes and generates questions
- System matches provided answers to question numbers
- Answers are validated against question options
- If answer invalid or missing: Uses default for that question
- Clarification context is passed to Phase 2 with provided answers

**Error handling**:
```bash
# Invalid answer code
/task-work TASK-e7f2 --answers="1:X"

# Output:
âš ï¸ Warning: Invalid answer 'X' for question 1
   Valid options: [M]inimal, [S]tandard, [C]omplete
   Using default: [S]tandard
```

### Complexity-Based Behavior

Clarification flags interact with task complexity:

| Complexity | Default Behavior | With --no-questions | With --with-questions |
|------------|------------------|---------------------|----------------------|
| 1-2 (Trivial) | Skip Phase 1.6 | Skip Phase 1.6 | Execute (quick mode, 15s timeout) |
| 3-4 (Simple) | Execute (quick mode, 15s) | Skip Phase 1.6 | Execute (quick mode, 15s) |
| 5+ (Complex) | Execute (full mode, blocking) | Skip Phase 1.6 | Execute (full mode, blocking) |

### Flag Precedence

When multiple clarification flags are present:

1. **--no-questions** (highest priority): Skips Phase 1.6 entirely
   - Overrides --with-questions, --defaults, --answers

2. **--answers**: Provides inline answers
   - Overrides --defaults
   - Compatible with --with-questions

3. **--defaults**: Uses all defaults
   - Overridden by --answers if both present

4. **--with-questions**: Forces execution
   - Only effective if --no-questions not present

**Examples**:
```bash
# --no-questions overrides everything
/task-work TASK-f7g2 --no-questions --with-questions
# Result: Phase 1.6 skipped (--no-questions wins)

# --answers overrides --defaults
/task-work TASK-h8j3 --defaults --answers="1:C"
# Result: Q1 uses answer C, others use defaults

# --with-questions forces execution
/task-work TASK-k3m7 --with-questions
# Result: Phase 1.6 executes even if complexity is 1-2
```

### Integration with Design-First Workflow

Clarification flags work with design-first workflow:

```bash
# Design-only with clarifications
/task-work TASK-n6p2 --design-only --answers="1:C 2:F"

# Design-only without clarifications (faster)
/task-work TASK-n6p2 --design-only --no-questions

# Implement-only skips Phase 1.6 automatically
/task-work TASK-n6p2 --implement-only
# (uses clarifications from design-only session)
```

**Note**: When using --implement-only, Phase 1.6 is always skipped because clarifications were already captured during the --design-only session.

### See Also

- [Phase 1.6 Specification](#phase-16-clarifying-questions-complexity-gated) - Complete phase workflow
- [Clarifying Questions Feature](../../tasks/backlog/clarifying-questions/) - Implementation details
- [Context C Templates](../../.claude/clarification/templates/context_c_implementation_planning.py) - Question templates

## Context7 MCP Integration (Library Documentation)

During task implementation, **automatically use Context7 MCP** to retrieve up-to-date library documentation when implementing with specific libraries or frameworks.

### When to Use Context7

Context7 should be invoked automatically during these phases:

1. **Phase 2: Implementation Planning**
   - When selecting libraries or frameworks for the implementation
   - When planning API usage patterns
   - When determining best practices for a library

2. **Phase 3: Implementation**
   - When implementing features using specific libraries
   - When unfamiliar with a library's API
   - When library documentation is needed for correct usage
   - When implementing patterns specific to a framework (React hooks, FastAPI patterns, etc.)

3. **Phase 4: Testing**
   - When writing tests using testing frameworks (pytest, Vitest, xUnit)
   - When setting up test fixtures or mocks
   - When implementing test patterns specific to the stack

### Context7 Workflow

**Step 1: Resolve Library ID**

Always resolve library name to Context7-compatible ID first:

```python
# Use mcp__context7__resolve-library-id tool
mcp__context7__resolve_library_id("react")
# Returns: /facebook/react or /facebook/react/v18.2.0
```

**Step 2: Get Library Documentation**

Use resolved ID to fetch documentation:

```python
# Use mcp__context7__get-library-docs tool
mcp__context7__get_library_docs(
  context7CompatibleLibraryID="/facebook/react",
  topic="hooks",              # Optional: focus area
  tokens=5000                 # Optional: max tokens (default: 5000)
)
```

### Examples by Stack

**React/TypeScript:**
- Libraries: "react", "next.js", "tailwindcss", "vitest", "playwright"
- Topics: "hooks", "routing", "styling", "testing"

**Python:**
- Libraries: "fastapi", "pytest", "pydantic", "langchain", "streamlit"
- Topics: "dependency-injection", "testing", "validation", "agents"

**.NET MAUI:**
- Libraries: "maui", "xamarin", "xunit", "moq"
- Topics: "mvvm", "data-binding", "navigation", "testing"

**TypeScript API:**
- Libraries: "nestjs", "typeorm", "jest", "supertest"
- Topics: "dependency-injection", "decorators", "testing", "validation"

### Integration Points in task-work Workflow

**Phase 2: Implementation Planning**
```
When task requires library usage:
1. Identify required libraries from requirements
2. Use Context7 to resolve library IDs
3. Fetch documentation for implementation approach
4. Incorporate library best practices into implementation plan
```

**Phase 3: Implementation**
```
When implementing with unfamiliar library APIs:
1. Use Context7 to get current documentation
2. Focus documentation on relevant topics (use `topic` parameter)
3. Implement according to latest library patterns
4. Verify implementation matches library best practices
```

**Phase 4: Testing**
```
When writing tests:
1. Use Context7 to get testing framework docs
2. Focus on testing patterns and assertions
3. Implement tests using framework best practices
```

### Best Practices

1. **Always resolve library ID first** - Don't assume library path format
2. **Use topic parameter** - Narrow documentation to relevant sections
3. **Limit token usage** - Default 5000 tokens is usually sufficient
4. **Cache library IDs** - Reuse resolved IDs within same task session
5. **Version awareness** - Use specific versions when available (/library/vX.Y.Z)
6. **Framework-specific patterns** - Always check library-specific patterns for the stack

### Error Handling

If Context7 library is not found:
- Proceed with general knowledge
- Document that library docs were unavailable
- Note in implementation for human review

### When NOT to Use Context7

- Standard language features (JavaScript, Python syntax)
- Well-established patterns (SOLID principles)
- General software engineering concepts
- Standard library functions (already in training data)

---

## ðŸŽ¯ EXECUTION PROTOCOL - START HERE IMMEDIATELY

When user runs `/task-work TASK-XXX [flags]`, **EXECUTE THIS EXACT SEQUENCE**:

### Step 0: Parse and Validate Flags (ENHANCED - Design-first + Documentation Levels)

**PARSE** command-line flags from user input:
```python
# Extract flags from command
design_only = "--design-only" in user_input or "-d" in user_input
implement_only = "--implement-only" in user_input or "-i" in user_input
micro = "--micro" in user_input

# Parse documentation level flag (TASK-036)
docs_flag = None
if "--docs=minimal" in user_input:
    docs_flag = "minimal"
elif "--docs=standard" in user_input:
    docs_flag = "standard"
elif "--docs=comprehensive" in user_input:
    docs_flag = "comprehensive"

# Parse mode flag (TASK-BDD-FIX1)
mode = "standard"  # Default mode
if "--mode=tdd" in user_input:
    mode = "tdd"
elif "--mode=bdd" in user_input:
    mode = "bdd"
elif "--mode=standard" in user_input:
    mode = "standard"
elif "--mode=" in user_input:
    # Invalid mode specified
    invalid_mode = user_input.split("--mode=")[1].split()[0]
    print(f"""
âŒ Error: Invalid mode '{invalid_mode}'

Valid modes:
  --mode=standard  # Default workflow (implementation + tests together)
  --mode=tdd       # Test-driven development (red â†’ green â†’ refactor)
  --mode=bdd       # Behavior-driven development (Gherkin scenarios)

Example:
  /task-work TASK-XXX --mode=tdd
    """)
    exit(1)
```

**VALIDATE** BDD mode requirements (TASK-BDD-FIX1):
```python
if mode == "bdd":
    from installer.core.commands.lib.feature_detection import supports_bdd

    if not supports_bdd():
        print("""
âŒ ERROR: BDD mode requires RequireKit installation

  Repository: https://github.com/requirekit/require-kit
  Installation:
    cd ~/Projects/require-kit
    ./installer/scripts/install.sh

  Alternative modes:
    /task-work {task_id} --mode=tdd      # Test-first development
    /task-work {task_id} --mode=standard # Default workflow
        """)
        exit(1)
```

**VALIDATE** flag mutual exclusivity:
```python
from installer.core.commands.lib.flag_validator import validate_flags

flags = {
    "design_only": design_only,
    "implement_only": implement_only,
    "micro": micro,
    "docs_flag": docs_flag  # TASK-036
}

try:
    validate_flags(flags)
except FlagConflictError as e:
    print(str(e))
    exit(1)
```

**DISPLAY** active flags (if any):
```python
# TASK-BDD-FIX1: Display active mode
mode_display = {
    "standard": "STANDARD (implementation + tests together)",
    "tdd": "TDD (test-driven development: red â†’ green â†’ refactor)",
    "bdd": "BDD (behavior-driven: Gherkin scenarios â†’ implementation)"
}
print(f"ðŸŽ¯ Development Mode: {mode_display[mode]}\n")

if design_only:
    print("ðŸŽ¨ Workflow Mode: DESIGN-ONLY (Phases 1-2.8)")
    print("   Task will stop at design approval checkpoint\n")
elif implement_only:
    print("ðŸš€ Workflow Mode: IMPLEMENT-ONLY (Phases 3-5)")
    print("   Using previously approved design\n")
elif micro:
    print("âš¡ Workflow Mode: MICRO-TASK (Streamlined)")
    print("   Lightweight workflow for trivial tasks\n")
else:
    print("ðŸ”„ Workflow Mode: STANDARD (All phases)")
    print("   Complete workflow with complexity-based checkpoints\n")

# TASK-036: Display documentation level if explicitly set
if docs_flag:
    print(f"ðŸ“„ Documentation Level: {docs_flag.upper()} (explicit override)")
    print(f"   Estimated time: {'8-12min' if docs_flag == 'minimal' else '12-18min' if docs_flag == 'standard' else '36+min'}\n")
```

**PROCEED** to Step 1 with flag context.

### Step 1: Load Task Context (REQUIRED - Multi-phase file resolution)

This step implements robust file resolution supporting descriptive filenames and automatic state detection.

#### Phase 1.1: Parse and Validate Task ID

**EXTRACT** task ID from user command:
```python
task_id = extract_task_id(user_input)  # e.g., "TASK-XXX" from "/task-work TASK-XXX"
```

**VALIDATE** task ID format:
- Must match pattern: `TASK-[A-Z0-9-]+` (e.g., TASK-001, TASK-BUG-001, TASK-003B-2)
- Reject if invalid: "Invalid task ID format: {task_id}. Expected format: TASK-XXX"

**DISPLAY**: "Loading task {task_id}..."

#### Phase 1.2: Multi-State File Search

**SEARCH** for task file across multiple states using glob patterns:

Search order (priority from highest to lowest):
1. `tasks/in_progress/{task_id}*.md` (expected location for active tasks)
2. `tasks/backlog/{task_id}*.md` (may need to transition to in_progress)
3. `tasks/blocked/{task_id}*.md` (may need to unblock and continue)
4. `tasks/in_review/{task_id}*.md` (edge case: re-work after review)

**Implementation pattern**:
```python
search_states = [
    ("in_progress", "tasks/in_progress"),
    ("backlog", "tasks/backlog"),
    ("blocked", "tasks/blocked"),
    ("in_review", "tasks/in_review")
]

matches = []
for state_name, state_dir in search_states:
    # Use Glob tool with pattern: {state_dir}/{task_id}*.md
    files = glob(f"{state_dir}/{task_id}*.md")
    for file in files:
        matches.append({
            "path": file,
            "state": state_name,
            "filename": extract_filename(file)
        })

    # Stop searching if found (priority order)
    if matches:
        break
```

**IMPORTANT**: Use Glob tool for file pattern matching, NOT bash find commands.

#### Phase 1.3: Handle Search Results

**CASE A: No matches found**
```python
if len(matches) == 0:
    # Task file not found in any state
    **DISPLAY** error report:
    ```
    âŒ Error: Task file not found

    Task ID: {task_id}
    Searched locations:
      - tasks/in_progress/{task_id}*.md
      - tasks/backlog/{task_id}*.md
      - tasks/blocked/{task_id}*.md
      - tasks/in_review/{task_id}*.md

    Possible causes:
    1. Task ID is incorrect or misspelled
    2. Task file has been deleted
    3. Task has been completed and archived

    Suggestions:
    - Verify task ID: /task-status (lists all tasks)
    - Check completed tasks: ls tasks/completed/
    - Create new task: /task-create "Task title"
    ```
    **EXIT** with error code
```

**CASE B: Single match found**
```python
if len(matches) == 1:
    task_file = matches[0]
    current_state = task_file["state"]
    file_path = task_file["path"]

    **DISPLAY**: "âœ… Found: {task_file['filename']} (state: {current_state})"

    # Proceed to Phase 1.4 (automatic state transition if needed)
```

**CASE C: Multiple matches found**
```python
if len(matches) > 1:
    # Multiple files match the pattern (edge case: duplicates)
    **DISPLAY** error report:
    ```
    âš ï¸  Warning: Multiple task files found

    Task ID: {task_id}
    Matches:
    {for each match:}
      {index}. {match['filename']} (state: {match['state']})

    This is unexpected and indicates duplicate task files.

    Recommendations:
    1. Review the duplicate files manually
    2. Delete or rename the incorrect file(s)
    3. Ensure only one file per task ID exists

    Locations:
    {for each match:}
      {match['path']}
    ```
    **EXIT** with error code
```

#### Phase 1.4: Automatic State Transition (if needed)

**IF** current_state != "in_progress":

```python
# Task file found in non-active state, needs transition
**DISPLAY** state transition prompt:
```
ðŸ”„ Task State Transition Required

Task: {task_id}
Current State: {current_state}
Required State: IN_PROGRESS (for task-work to execute)

File: {file_path}

Automatic transition will:
1. Move file: {current_state}/{filename} â†’ in_progress/{filename}
2. Update task metadata (status, updated timestamp)
3. Preserve all task content and history

Proceed with state transition? [Y/n]:
```

**WAIT** for user confirmation (default: Yes after 5 seconds)

**IF** user confirms (or timeout):
    1. **READ** task file to extract frontmatter and content
    2. **UPDATE** frontmatter metadata:
       ```yaml
       status: in_progress
       updated: {current_timestamp_iso8601}
       previous_state: {current_state}
       state_transition_reason: "Automatic transition for task-work execution"
       ```
    3. **WRITE** updated file to `tasks/in_progress/{filename}`
    4. **DELETE** old file from `tasks/{current_state}/{filename}`
    5. **DISPLAY**: "âœ… Transitioned {task_id} from {current_state} to IN_PROGRESS"
    6. **UPDATE** variables:
       ```python
       file_path = f"tasks/in_progress/{filename}"
       current_state = "in_progress"
       ```

**IF** user declines:
    **DISPLAY**: "âŒ State transition declined. Cannot execute task-work on {current_state} tasks."
    **EXIT** with error code

**ELSE** (already in_progress):
    # No transition needed, proceed directly
    **DISPLAY**: "âœ… Task is already IN_PROGRESS"

#### Phase 1.5: Load Task Context

**READ** task file from final location: `{file_path}`

**FEATURE DETECTION** - Check for require-kit:
```python
from lib.feature_detection import supports_requirements, supports_epics, supports_bdd

REQUIREMENTS_AVAILABLE = supports_requirements()  # True if require-kit installed
```

**EXTRACT** required context:
```python
task_context = {
    "task_id": task_id,
    "file_path": file_path,
    "state": current_state,

    # Core fields (always available)
    "title": frontmatter.title,
    "priority": frontmatter.priority,
    "assignee": frontmatter.assignee,
    "description": extract_description(content),
    "acceptance_criteria": extract_acceptance_criteria(content),
    "implementation_notes": extract_implementation_notes(content)
}

# Conditional fields (only if require-kit installed)
if REQUIREMENTS_AVAILABLE:
    task_context["requirements"] = frontmatter.requirements or []      # List of REQ-XXX IDs
    task_context["bdd_scenarios"] = frontmatter.bdd_scenarios or []    # List of BDD-XXX IDs
    task_context["epic"] = frontmatter.epic or None                    # EPIC-XXX
    task_context["feature"] = frontmatter.feature or None              # FEAT-XXX
else:
    # Gracefully skip requirements features if require-kit not installed
    task_context["requirements"] = []
    task_context["bdd_scenarios"] = []
    task_context["epic"] = None
    task_context["feature"] = None
```

**IF BDD MODE**: Load Gherkin scenarios from RequireKit:
```python
# Check if BDD mode is active
if mode == "bdd":
    # RequireKit already validated in Step 0 (TASK-BDD-FIX1)
    # bdd_scenarios field already loaded above

    if not task_context["bdd_scenarios"]:
        print("""
ERROR: BDD mode requires linked Gherkin scenarios

  Task frontmatter must include bdd_scenarios field:

    ---
    id: {task_id}
    title: {title}
    bdd_scenarios: [BDD-001, BDD-002]  â† Add this
    ---

  Generate scenarios in RequireKit:
    cd ~/Projects/require-kit
    /formalize-ears REQ-XXX
    /generate-bdd REQ-XXX

  Or use alternative modes:
    /task-work {task_id} --mode=tdd
    /task-work {task_id} --mode=standard
        """)
        sys.exit(1)

    # Load Gherkin scenario content from RequireKit
    scenarios = []
    requirekit_path = Path.home() / "Projects" / "require-kit"

    for scenario_id in task_context["bdd_scenarios"]:
        # Find scenario file in RequireKit
        scenario_file = requirekit_path / "docs" / "bdd" / f"{scenario_id}.feature"

        if not scenario_file.exists():
            print(f"""
ERROR: Scenario {scenario_id} not found at {scenario_file}

  Generate scenario in RequireKit:
    cd {requirekit_path}
    /generate-bdd REQ-XXX

  Verify scenarios exist:
    ls {requirekit_path}/docs/bdd/{scenario_id}.feature
            """)
            sys.exit(1)

        # Read Gherkin content
        with open(scenario_file) as f:
            scenario_content = f.read()

        scenarios.append({
            "id": scenario_id,
            "file": str(scenario_file),
            "content": scenario_content
        })

    # Add scenarios to task context
    task_context["gherkin_scenarios"] = scenarios

    # Detect BDD framework for this project
    task_context["bdd_framework"] = detect_bdd_framework(Path.cwd())

    # Display loaded scenarios
    print(f"\nâœ… Loaded {len(scenarios)} BDD scenarios from RequireKit:")
    for s in scenarios:
        print(f"   â€¢ {s['id']}")
    print(f"   Framework: {task_context['bdd_framework']}\n")
```

**BDD FRAMEWORK DETECTION FUNCTION**:
```python
def detect_bdd_framework(project_path: Path) -> str:
    """
    Detect BDD testing framework based on project files.

    Args:
        project_path: Path to project root directory

    Returns:
        Framework name (pytest-bdd, specflow, cucumber-js, cucumber)
    """
    # Python - check requirements.txt
    if (project_path / "requirements.txt").exists():
        with open(project_path / "requirements.txt") as f:
            if "pytest-bdd" in f.read():
                return "pytest-bdd"

    # Python - check pyproject.toml
    if (project_path / "pyproject.toml").exists():
        with open(project_path / "pyproject.toml") as f:
            if "pytest-bdd" in f.read():
                return "pytest-bdd"

    # .NET - check .csproj files
    csproj_files = list(project_path.glob("*.csproj"))
    if csproj_files:
        with open(csproj_files[0]) as f:
            if "SpecFlow" in f.read():
                return "specflow"

    # TypeScript/JavaScript - check package.json
    if (project_path / "package.json").exists():
        with open(project_path / "package.json") as f:
            import json
            pkg = json.load(f)
            dev_deps = pkg.get("devDependencies", {})
            if "@cucumber/cucumber" in dev_deps:
                return "cucumber-js"

    # Ruby - check Gemfile
    if (project_path / "Gemfile").exists():
        with open(project_path / "Gemfile") as f:
            if "cucumber" in f.read():
                return "cucumber"

    # Default fallback
    return "pytest-bdd"
```

**VALIDATE** essential fields exist:
- `title`: Must be present
- `acceptance_criteria`: At least one criterion required
- Warn if missing: `requirements`, `bdd_scenarios` (only if require-kit installed)

**DISPLAY** loaded context summary:
```
ðŸ“‹ Task Context Loaded

ID: {task_id}
Title: {title}
State: {state}
Priority: {priority}

{if REQUIREMENTS_AVAILABLE:}
Requirements: {len(requirements)} linked ({', '.join(requirements[:3])}{' ...' if len > 3})
BDD Scenarios: {len(bdd_scenarios)} linked
Epic: {epic or 'None'}
Feature: {feature or 'None'}
{else:}
[Requirements features not available - install require-kit for EARS/BDD/Epic support]
{endif}

Acceptance Criteria: {len(acceptance_criteria)} items
```

**PROCEED** to Step 2 (Detect Technology Stack)

### Step 2: Detect Technology Stack (REQUIRED - 10 seconds)

**READ** `.claude/settings.json` and extract `project.template` value.

If file exists: Use `project.template` value
If file not exists: Set stack to "default"

**DISPLAY**: "ðŸ” Detected stack: {stack}"

### Step 2.5: Determine Documentation Level (NEW - TASK-036)

**PURPOSE**: Establish documentation verbosity based on configuration hierarchy

**Configuration Hierarchy** (highest to lowest priority):
1. Command-line flag: `--docs=minimal|standard|comprehensive`
2. Force-comprehensive triggers (security, compliance, breaking changes)
3. Settings.json default: `.claude/settings.json` â†’ `documentation.default_level`
4. Default: `minimal` (use `--docs=standard` to lift)

**STEP 1: Load Configuration**

```python
# Read documentation settings from .claude/settings.json
try:
    settings = read_json(".claude/settings.json")
    doc_config = settings.get("documentation", {})
    enabled = doc_config.get("enabled", True)
    default_level = doc_config.get("default_level", "auto")
    force_triggers = doc_config.get("force_comprehensive", {}).get("triggers", {})
except FileNotFoundError:
    # No settings file - use defaults
    enabled = True
    default_level = "auto"
    force_triggers = {}
```

**STEP 2: Check Force-Comprehensive Triggers**

```python
task_text = (task_context.get("title", "") + " " + task_context.get("description", "")).lower()

# Check triggers from settings or use defaults
security_keywords = force_triggers.get("security_keywords", ["auth", "password", "encryption", "security"])
compliance_keywords = force_triggers.get("compliance_keywords", ["gdpr", "hipaa", "compliance", "audit"])
breaking_keywords = force_triggers.get("breaking_changes", ["breaking", "migration", "deprecated"])

force_comprehensive = (
    any(kw in task_text for kw in security_keywords) or
    any(kw in task_text for kw in compliance_keywords) or
    any(kw in task_text for kw in breaking_keywords)
)
```

**STEP 3: Apply Configuration Hierarchy**

```python
documentation_level = None
reason = None

# Priority 1: Command-line flag (highest)
if docs_flag:
    documentation_level = docs_flag
    reason = f"explicit flag (--docs={docs_flag})"

# Priority 2: Force-comprehensive triggers
elif force_comprehensive:
    documentation_level = "comprehensive"
    reason = "force trigger (security/compliance/breaking keywords)"

# Priority 3: Settings.json default_level
elif default_level != "auto":
    documentation_level = default_level
    reason = "settings.json default"

# Priority 4: Default to minimal (lowest)
else:
    documentation_level = "minimal"
    reason = "default (use --docs=standard to lift)"
```

**STEP 4: Store in Context & Display**

```python
# Add to task_context for agent invocations
task_context["documentation_level"] = documentation_level

**DISPLAY**:
ðŸ“„ Documentation Level: {documentation_level.upper()}
   Reason: {reason}
   Files: {2 if documentation_level != 'comprehensive' else '13+'} files
   Estimated: {8-12 if documentation_level == 'minimal' else 12-18 if documentation_level == 'standard' else 36+} minutes
```

**PROCEED** to Step 3 (Select Agents)

### Step 3: Agent Discovery (Automatic)

**Agent selection is DYNAMIC** based on metadata matching. The system:

1. Analyzes task context (stack, phase, keywords from description)
2. Scans all agent sources (Local > User > Global > Template)
3. Returns best match based on metadata (stack, phase, capabilities, keywords)

**No action required** - Discovery happens automatically during each phase.

**See**: [Agent Discovery System](#agent-discovery-system) for complete details on:
- Discovery sources and precedence
- Metadata requirements
- Template override behavior
- Source indicators (ðŸ“ ðŸ“¦ ðŸŒ)
- Troubleshooting

**Agent selection results** are shown in the invocation log after task completion.

### Step 3.5: Initialize Tracking and Validation (NEW - TASK-ENF2, TASK-ENF4)

**INITIALIZE INVOCATION TRACKER AND PHASE GATE VALIDATOR**:
```python
from installer.core.commands.lib import (
    AgentInvocationTracker,
    add_pending_phases,
    PhaseGateValidator
)

# Initialize tracker for execution visibility
tracker = AgentInvocationTracker()
add_pending_phases(tracker, workflow_mode="standard")  # or "micro"

# Initialize phase gate validator
validator = PhaseGateValidator(tracker)
```

**Purpose**:
- **Tracker**: Records which agents are invoked, their sources, and execution status
- **Validator**: Ensures agents are properly invoked before allowing phase progression

**See**:
- TASK-ENF2: Agent invocation tracking
- TASK-ENF4: Phase gate validation checkpoints

### Step 4: INVOKE TASK TOOL FOR EACH PHASE (REQUIRED - DO NOT SKIP)

**âš ï¸ CRITICAL: YOU MUST USE THE TASK TOOL. DO NOT ATTEMPT TO DO THE WORK YOURSELF.**

#### Phase 1: Requirements Analysis *(Require-Kit Only)*

**SKIPPED IN GUARDKIT**: GuardKit uses task descriptions and acceptance criteria directly without formal requirements analysis.

**Why skipped**: GuardKit is lightweight - no EARS notation or formal BDD generation needed.

**For formal requirements**: Use [require-kit](https://github.com/requirekit/require-kit) which provides:
- EARS notation requirements analysis
- BDD/Gherkin scenario generation
- Full requirements traceability

**GuardKit workflow**: Proceed to Phase 1.6 (Clarifying Questions), then Phase 2 (Implementation Planning).

#### Phase 1.6: Clarifying Questions (Complexity-Gated)

**Purpose**: Ask targeted clarifying questions before making assumptions in implementation planning.

**Trigger**: After context loading (Phase 1.5), before implementation planning (Phase 2)

**Complexity Gating**:

| Complexity | Behavior |
|------------|----------|
| 1-2 (Trivial) | Skip - proceed directly to Phase 2 |
| 3-4 (Simple) | Quick mode - 15s timeout, then use defaults |
| 5+ (Complex) | Full mode - blocking, wait for user response |

**Workflow**:

**IF** `--no-questions` flag is set:
```
DISPLAY: "â­ï¸  Clarification skipped (--no-questions flag)"
Skip to Phase 2
```

**ELSE IF** `--implement-only` flag is set:
```
DISPLAY: "â­ï¸  Clarification skipped (using saved design)"
Skip to Phase 2
```

**ELSE**:

**INVOKE** Task tool:
```
subagent_type: "clarification-questioner"
description: "Collect implementation planning clarifications for TASK-{task_id}"
prompt: "Execute clarification for TASK-{task_id}.

CONTEXT TYPE: implementation_planning

TASK CONTEXT:
  Title: {task_context.title}
  Description: {task_context.description}
  Complexity: {task_context.complexity}/10
  Acceptance Criteria: {task_context.acceptance_criteria}
  Stack: {detected_stack}

FLAGS:
  --no-questions: {flags.no_questions}
  --with-questions: {flags.with_questions}
  --defaults: {flags.defaults}
  --answers: {flags.answers}

Execute clarification based on complexity gating:
- Complexity 1-2: Skip unless --with-questions
- Complexity 3-4: Quick mode (15s timeout)
- Complexity 5+: Full mode (blocking)

Return ClarificationContext with user decisions."
```

**WAIT** for agent completion

**STORE** `clarification_context` for Phase 2 prompt

**DISPLAY**:
```
âœ… Phase 1.6: Clarification complete
  Mode: {clarification_context.mode}
  Decisions: {clarification_context.answered_count}
  Defaults used: {len(clarification_context.assumed_defaults)}
```

**Command-Line Flags**:

| Flag | Effect |
|------|--------|
| `--no-questions` | Skip Phase 1.6 entirely |
| `--with-questions` | Force Phase 1.6 even for complexity 1-2 |
| `--defaults` | Use all defaults without prompting |
| `--answers="..."` | Provide answers inline (CI/CD automation) |

**Example Flow**:

```
/task-work TASK-a3f8

Phase 1.5: Loading context...
Phase 1.6: Clarifying Questions (complexity: 5)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ðŸ“‹ CLARIFYING QUESTIONS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Q1. Implementation Scope
    How comprehensive should this implementation be?

    [M]inimal - Core functionality only
    [S]tandard - With error handling (DEFAULT)
    [C]omplete - Production-ready with edge cases

    Your choice [M/S/C]: S

Q2. Testing Approach
    What testing strategy?

    [U]nit tests only
    [I]ntegration tests included (DEFAULT)
    [F]ull coverage (unit + integration + e2e)

    Your choice [U/I/F]: I

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ“ Recorded 2 decisions
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Phase 2: Planning implementation with clarifications...
```

**Quick Mode Timeout Behavior**:

For complexity 3-4 tasks (simple):
- Display questions with 15-second countdown
- If user responds within 15s: Use their answers
- If timeout expires: Automatically use default values
- Display: "â±ï¸ Timeout - using defaults for remaining questions"

For complexity 5+ tasks (complex):
- No timeout - blocking wait for user response
- User must answer or cancel with Ctrl+C

**Skip Conditions**:

Phase 1.6 is skipped when:
- `--no-questions` flag is present
- Task complexity is 1-2 (trivial)
- `--design-only` flag is present (design-first workflow)
- Task is in DESIGN_APPROVED state (implement-only)

**See**: [Clarifying Questions Feature](../../tasks/backlog/clarifying-questions/) for complete implementation details.

#### Phase 2: Implementation Planning

**DISPLAY INVOCATION MESSAGE**:
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ðŸ¤– INVOKING AGENT: {selected_planning_agent_from_table}
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Phase: 2 (Implementation Planning)
Model: Sonnet (Deep understanding of architecture and design patterns)
Stack: {detected_stack}
Specialization:
  - Architecture design and pattern selection
  - Technology-specific implementation strategy
  - Complexity and risk assessment

Starting agent execution...
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**INVOKE** Task tool with documentation context and clarification decisions:
```
subagent_type: "{selected_planning_agent_from_table}"
description: "Plan implementation for TASK-XXX"
prompt: "<AGENT_CONTEXT>
documentation_level: {documentation_level}
complexity_score: {task_context.complexity}
task_id: {task_id}
stack: {stack}
phase: 2
{if clarification_context:}
clarification_context: {clarification_context}
{endif}
</AGENT_CONTEXT>

Design {stack} implementation approach for {task_id}.
Include architecture decisions, pattern selection, and component structure.
Consider {stack}-specific best practices and testing strategies.

{if clarification_context:}
CLARIFICATION CONTEXT (from Phase 1.6):
User provided the following clarifications:
{for decision in clarification_context.explicit_decisions:}
  - {decision.question_text}: {decision.answer_display}
{endfor}

Defaults applied (user did not override):
{for decision in clarification_context.assumed_defaults:}
  - {decision.question_text}: {decision.answer_display} (default)
{endfor}

Use these clarifications to inform your implementation plan.
{endif}

{if mode == 'bdd':}
BDD MODE CONTEXT:
- BDD Scenarios loaded: {len(task_context.get('gherkin_scenarios', []))} scenarios
- Framework: {task_context.get('bdd_framework')}
- Scenarios:
{for scenario in task_context.get('gherkin_scenarios', []):}
  â€¢ {scenario['id']}: {scenario['content'][:200]}...
{endfor}

Implementation plan should:
1. Account for step definitions matching these scenarios
2. Structure code to facilitate BDD testing
3. Map Given/When/Then steps to implementation components
{endif}

DOCUMENTATION BEHAVIOR (documentation_level={documentation_level}):
- minimal: Return plan as structured data (file list, phases, estimates). CONSTRAINT: Generate ONLY 2 files maximum.
- standard: Return plan with brief architecture notes and key decisions. CONSTRAINT: Generate ONLY 2 files maximum.
- comprehensive: Generate detailed implementation guide with ADRs and diagrams (13+ files allowed)

Output: Implementation plan matching documentation level expectations."
```

**WAIT** for agent to complete before proceeding.

**DISPLAY COMPLETION MESSAGE**:
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âœ… AGENT COMPLETED: {selected_planning_agent_from_table}
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Duration: {phase_2_duration_seconds}s
Files to create: {planned_file_count}
Architecture patterns identified: {pattern_count}
Risk factors: {risk_level}
Status: Implementation plan generated successfully

Proceeding to Phase 2.5A...
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**PHASE GATE VALIDATION** (NEW - TASK-ENF4):
```python
from installer.core.commands.lib import PhaseGateValidator, ValidationError

try:
    validator.validate_phase_completion("2", "Implementation Planning")
except ValidationError as e:
    print(str(e))
    move_task_to_blocked(task_id, reason="Phase 2 gate violation - agent not invoked")
    exit(1)
```

**IF validation passes**: Proceed to Phase 2.5A
**IF validation fails**: Task moved to BLOCKED, execution stops

#### Phase 2.5A: Pattern Suggestion (Conditional - Skip for simple tasks)

**STEP 1: Evaluate Skip Conditions**

Before invoking Design Patterns MCP, evaluate whether pattern suggestions would add value:

```python
def should_invoke_design_patterns_mcp(task_context):
    """Determine if design patterns MCP adds value for this task."""

    # Get task metadata
    complexity = task_context.get("complexity", 5)
    task_type = task_context.get("task_type", "feature")
    description = task_context.get("description", "")
    title = task_context.get("title", "")

    # Combine title and description for pattern matching
    task_text = f"{title} {description}".lower()

    # Skip Condition 1: Simple tasks (complexity â‰¤3)
    if complexity <= 3:
        return False, f"complexity {complexity} <= 3 (simple task)"

    # Skip Condition 2: Bug fixes
    if task_type == "bugfix":
        return False, "task_type is 'bugfix' (no new architecture needed)"

    # Skip Condition 3: Task already references a known pattern
    known_patterns = [
        "singleton", "repository", "factory", "strategy", "observer",
        "adapter", "decorator", "facade", "command", "mediator",
        "builder", "prototype", "chain of responsibility", "state",
        "template method", "visitor", "memento", "iterator"
    ]

    for pattern in known_patterns:
        if pattern in task_text:
            return False, f"task references '{pattern}' pattern"

    # All checks passed - invoke MCP
    return True, None
```

**EVALUATE** skip conditions:

```python
should_invoke, skip_reason = should_invoke_design_patterns_mcp(task_context)
```

**IF** should_invoke == False:

**DISPLAY** skip message:
```
â­ï¸  Skipping Pattern Suggestion (Phase 2.5A)
   Reason: {skip_reason}

   Proceeding to Phase 2.5B...
```

**PROCEED** directly to Phase 2.5B (Architectural Review)

---

**STEP 2: Invoke MCP (if not skipped)**

**IF** should_invoke == True AND Design Patterns MCP is available (check for mcp__design-patterns tools):

**QUERY** Design Patterns MCP using problem description from implementation plan:
```
Use find_patterns with REQUIRED programmingLanguage parameter:

mcp__design-patterns__find_patterns(
  query: "{problem description from task} for {stack} application",
  programmingLanguage: "{map stack to language: maui->csharp, react->typescript, python->python, typescript-api->typescript, dotnet-microservice->csharp}",
  maxResults: 3  // Limit to top 3 to reduce noise
)

Example for MAUI stack:
query: "Repository pattern with error handling using ErrorOr for database write operations in C# .NET MAUI mobile application"
programmingLanguage: "csharp"
maxResults: 3

Parse MCP response to extract:
- Recommended patterns (with confidence scores)
- Pattern categories (Resilience, Performance, etc.)
- Why each pattern is recommended
- Implementation guidance for {stack}

**FILTER RESULTS**: Skip patterns that don't match detected stack (e.g., React patterns for MAUI tasks)
```

**DISPLAY** pattern recommendations (if any found):
```
ðŸŽ¯ Design Pattern Recommendations

Based on task requirements and constraints:

1. **Circuit Breaker Pattern** (Confidence: 95%)
   Category: Resilience
   Why: Handles external API failures, enforces timeout constraints
   Stack guidance: {stack-specific implementation notes from MCP}

2. **Retry Pattern** (Confidence: 82%)
   Category: Resilience
   Why: Handles transient failures, works with Circuit Breaker
   Stack guidance: {stack-specific implementation notes from MCP}

[Additional patterns if relevant...]
```

**IF** no Design Patterns MCP available:

**DISPLAY**:
```
â­ï¸  Skipping Pattern Suggestion (Phase 2.5A)
   Reason: Design Patterns MCP not available

   Proceeding to Phase 2.5B...
```

**PROCEED** to Phase 2.5B.

#### Phase 2.5B: Architectural Review (Catch design issues early)

**DISPLAY INVOCATION MESSAGE**:
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ðŸ¤– INVOKING AGENT: architectural-reviewer
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Phase: 2.5B (Architectural Review)
Model: Sonnet (Expert-level architecture analysis)
Stack: {detected_stack}
Specialization:
  - SOLID principles verification
  - Design pattern validation
  - Risk and complexity assessment

Starting agent execution...
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**INVOKE** Task tool with documentation context:
```
subagent_type: "architectural-reviewer"
description: "Review architecture for TASK-XXX"
prompt: "<AGENT_CONTEXT>
documentation_level: {documentation_level}
complexity_score: {task_context.complexity}
task_id: {task_id}
stack: {stack}
phase: 2.5
</AGENT_CONTEXT>

Review the implementation plan from Phase 2 for {task_id}.
Evaluate against SOLID principles, DRY principle, and YAGNI principle.
Check for: single responsibility, proper abstraction, unnecessary complexity.
Score each principle (0-100) and provide specific recommendations.

PATTERN CONTEXT (if Design Patterns MCP was queried):
{Include pattern recommendations from Phase 2.5A}
- Validate if suggested patterns are appropriate
- Check if implementation plan aligns with pattern best practices
- Identify if patterns are over-engineered for the requirements

DOCUMENTATION BEHAVIOR (documentation_level={documentation_level}):
- minimal: Return scores and critical issues only (structured data). CONSTRAINT: Generate ONLY 2 files maximum.
- standard: Return scores with brief explanations and recommendations. CONSTRAINT: Generate ONLY 2 files maximum.
- comprehensive: Generate detailed architecture review report with rationale (13+ files allowed)

Approval thresholds:
- â‰¥80/100: Auto-approve (proceed to Phase 3)
- 60-79/100: Approve with recommendations
- <60/100: Reject (revise design)

See installer/core/agents/architectural-reviewer.md for documentation level specifications."
```

**WAIT** for agent to complete before proceeding.

**DISPLAY COMPLETION MESSAGE**:
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âœ… AGENT COMPLETED: architectural-reviewer
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Duration: {phase_25b_duration_seconds}s
SOLID Score: {solid_score}/100
DRY Score: {dry_score}/100
YAGNI Score: {yagni_score}/100
Overall Recommendation: {recommendation}
Status: Architectural review complete

Proceeding to Phase 2.7...
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**PHASE GATE VALIDATION** (NEW - TASK-ENF4):
```python
try:
    validator.validate_phase_completion("2.5B", "Architectural Review")
except ValidationError as e:
    print(str(e))
    move_task_to_blocked(task_id, reason="Phase 2.5B gate violation - agent not invoked")
    exit(1)
```

**IF validation passes**: Proceed to Phase 2.7
**IF validation fails**: Task moved to BLOCKED, execution stops

#### Phase 2.7: Complexity Evaluation (NEW - Auto-proceed mode routing)

**INVOKE** Task tool:
```
subagent_type: "complexity-evaluator"
description: "Evaluate implementation complexity for TASK-XXX"
prompt: "Evaluate implementation complexity for TASK-XXX using the implementation plan from Phase 2.

         Extract and analyze:
         - File count (files to create/modify)
         - Design patterns mentioned
         - External dependencies (APIs, databases, services)
         - Risk indicators (security, schema changes, performance)

         Calculate complexity score (1-10 scale) based on:
         - File complexity factor (0-3 points)
         - Pattern familiarity factor (0-2 points)
         - Risk level factor (0-3 points)

         Detect force-review triggers:
         - User flag (--review)
         - Security keywords
         - Breaking changes
         - Schema changes
         - Hotfix

         Route to review mode:
         - Score 1-3: AUTO_PROCEED (display summary, proceed to Phase 3)
         - Score 4-6: QUICK_OPTIONAL (offer optional checkpoint)
         - Score 7-10 or triggers: FULL_REQUIRED (mandatory Phase 2.6)

         Output: ComplexityScore with routing decision and human-readable summary."
```

**WAIT** for agent to complete before proceeding.

**EVALUATE** complexity evaluation result:

```python
complexity_result = extract_complexity_result(phase_27_output)
review_mode = complexity_result.review_mode  # AUTO_PROCEED, QUICK_OPTIONAL, or FULL_REQUIRED
```

**IF** review_mode == AUTO_PROCEED:
- Display complexity summary
- Automatically proceed to Phase 3 (no human intervention)

**ELSE IF** review_mode == QUICK_OPTIONAL:
- Display complexity summary with optional checkpoint prompt
- Offer user choice: [A]pprove, [R]eview, [Enter] to auto-approve
- Default to proceed after 10 seconds timeout
- If user chooses [R]eview, proceed to Phase 2.6

**ELSE IF** review_mode == FULL_REQUIRED:
- Display detailed complexity summary
- Mandatory Phase 2.6 human checkpoint (see below)

#### Phase 2.6: Human Checkpoint (Optional - Triggered by complexity evaluation or critical tasks)

**NOTE**: This phase is now triggered by Phase 2.7 complexity evaluation:
- **Mandatory**: If complexity score 7-10 OR force-review triggers detected
- **Optional**: If complexity score 4-6 AND user chooses to review
- **Skipped**: If complexity score 1-3 (auto-proceed)

**Human checkpoint is triggered by Phase 2.7 complexity evaluation**:

```python
# Automatic triggers from Phase 2.7
if complexity_result.review_mode == FULL_REQUIRED:
    trigger_checkpoint = True
    checkpoint_reason = "Complexity score 7-10 or force-review triggers"
elif complexity_result.review_mode == QUICK_OPTIONAL and user_chose_review:
    trigger_checkpoint = True
    checkpoint_reason = "User requested review (optional)"
else:
    trigger_checkpoint = False  # AUTO_PROCEED - skip to Phase 3
```

**IF TRIGGERED**, display interactive checkpoint:

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ðŸ” PHASE 2.6 - HUMAN CHECKPOINT REQUIRED
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

TASK: {TASK-ID} - {Title}

COMPLEXITY EVALUATION (Phase 2.7):
  Score: {complexity_score}/10 ({review_mode})
  Triggers: {List of force-review triggers}
  Reason: {checkpoint_reason}

ARCHITECTURAL REVIEW (Phase 2.5B):
  Score: {arch_score}/100 ({arch_status})
  Issues: {issue_count}
  {List of critical issues and recommendations}

ESTIMATED FIX TIME: {minutes} minutes (design adjustment)

OPTIONS:
1. [A]pprove - Proceed with current design
2. [R]evise - Apply recommendations and re-review
3. [V]iew - Show full architectural review report
4. [C]omplexity - Show detailed complexity breakdown
5. [D]iscuss - Escalate to software-architect

Your choice (A/R/V/C/D):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**WAIT** for human decision:
- **Approve**: Continue to Phase 3 (implementation)
- **Revise**: Loop back to Phase 2 (planning) with feedback
- **View**: Display full architectural review report, then prompt again
- **Complexity**: Display detailed complexity breakdown, then prompt again
- **Discuss**: Invoke software-architect agent for consultation

**IF NOT TRIGGERED** (auto-proceed from Phase 2.7):
- Display complexity summary (score 1-3)
- Automatically proceed to Phase 3 with no human intervention

#### Phase 2.7: Implementation Plan Generation & Complexity Evaluation (ENHANCED)

**PURPOSE**: Generate structured implementation plan and evaluate complexity to route to appropriate review mode

**ENHANCEMENTS (TASK-027 - Markdown Plans)**:
- **Dual Format Support**: Generates both Markdown (`implementation_plan.md`) and JSON (`implementation_plan.json`) formats
- **Human-Readable Plans**: Markdown format improves readability for Phase 2.8 checkpoint display
- **Backward Compatibility**: JSON format preserved for automated processing
- **Git-Friendly**: Markdown plans are easier to review in version control diffs
- **Plan Location**: Both files saved to `docs/state/{task_id}/`

**INVOKE** Task tool:
```
subagent_type: "task-manager"
description: "Generate implementation plan and evaluate complexity for TASK-XXX"
prompt: "Execute Phase 2.7 for TASK-XXX:

         STEP 1: PARSE IMPLEMENTATION PLAN
         - Parse Phase 2 planning output into structured ImplementationPlan
         - Extract: files to create/modify, patterns, dependencies, risks, phases
         - Use stack-specific parser if available, fallback to generic
         - Save to: docs/state/{task_id}/implementation_plan.json

         STEP 2: CALCULATE COMPLEXITY SCORE
         - Use ComplexityCalculator to evaluate plan (1-10 scale)
         - Factors: file count, pattern familiarity, risk level, dependencies
         - Save to: docs/state/{task_id}/complexity_score.json

         STEP 3: DETECT FORCE-REVIEW TRIGGERS
         - Security keywords (auth, password, encryption, etc.)
         - Schema changes (database migrations)
         - Breaking changes (public API modifications)
         - User flag (--review command-line option)
         - Hotfix or production tags

         STEP 4: DETERMINE REVIEW MODE
         - Score 1-3 + no triggers â†’ AUTO_PROCEED
         - Score 4-6 + no triggers â†’ QUICK_OPTIONAL
         - Score 7-10 OR any trigger â†’ FULL_REQUIRED

         STEP 5: RETURN RESULTS
         - ComplexityScore with review_mode
         - ImplementationPlan path
         - Force triggers list (if any)

         Stack: {detected_stack}
         Phase 2 Output: {phase_2_planning_output}
         Task Metadata: {task_frontmatter}"
```

**WAIT** for agent to complete before proceeding.

**EXTRACT** Phase 2.7 results:
```python
complexity_score = extract_complexity_score(phase_27_output)
review_mode = complexity_score.review_mode  # AUTO_PROCEED | QUICK_OPTIONAL | FULL_REQUIRED
plan_path = f"docs/state/{task_id}/implementation_plan.json"
triggers = complexity_score.forced_review_triggers
```

**DISPLAY** Phase 2.7 summary:
```
Phase 2.7 Complete: Plan Generated & Complexity Evaluated

Plan saved: {plan_path}
Complexity Score: {complexity_score.total_score}/10 ({complexity_score.level})
Review Mode: {review_mode}
{If triggers: "Force Triggers: " + ", ".join(triggers)}
```

#### Phase 2.8: Human Plan Checkpoint (ENHANCED - Rich Display & Interactive Modification)

**PURPOSE**: Route to appropriate review based on complexity score from Phase 2.7, with rich visual display and interactive plan modification capabilities.

**ENHANCEMENTS (TASK-028, TASK-029)**:
- **Rich Visual Display**: Human-readable plan summary with file changes, dependencies, risks, effort
- **Markdown & JSON Support**: Loads plans from both `implementation_plan.md` and `implementation_plan.json`
- **Interactive Modification**: [M]odify option for adjusting plan before implementation
- **Version Management**: Automatic plan versioning with timestamped backups
- **Undo Support**: Revert to previous plan versions during modification

**ROUTE** based on review_mode from Phase 2.7:

**IF** review_mode == AUTO_PROCEED:
```
Display auto-proceed summary:

  Auto-Proceed Mode (Low Complexity)

  Complexity: {score}/10 (Simple task)
  Files: {file_count} file(s)
  Tests: {test_count} tests planned
  Estimated: ~{duration} minutes

  Automatically proceeding to implementation (no review needed)...

Update task metadata:
  auto_approved: true
  approved_by: "system"
  approved_at: {current_timestamp}
  review_mode: "auto_proceed"

Proceed immediately to Phase 3 (Implementation)
```

**ELSE IF** review_mode == QUICK_OPTIONAL:

**INVOKE** Task tool:
```
subagent_type: "task-manager"
description: "Execute quick review checkpoint for TASK-XXX"
prompt: "Execute Phase 2.8 Quick Review for TASK-XXX:

         STEP 1: LOAD CONTEXT
         - Load ImplementationPlan from {plan_path}
         - Load ComplexityScore from complexity_score.json
         - Extract summary information

         STEP 2: DISPLAY QUICK REVIEW CARD
         - Complexity score and level
         - File count summary
         - Pattern summary
         - Estimated duration
         - Brief risk summary (if any)

         STEP 3: START 10-SECOND COUNTDOWN
         - Use QuickReviewHandler from review_modes.py
         - Display countdown timer (10...9...8...)
         - Listen for user input:
           * ENTER pressed â†’ Return 'escalate' (escalate to full review)
           * 'c' pressed â†’ Return 'cancel' (cancel task, move to backlog)
           * Timeout (no input) â†’ Return 'timeout' (auto-approve, proceed to Phase 3)

         STEP 4: UPDATE TASK METADATA
         - Record review decision
         - Update timestamps
         - Set proceed_to_phase_3 flag accordingly

         Return result: {'action': 'timeout'|'escalate'|'cancel', 'duration': seconds}"
```

**WAIT** for result

**IF** result.action == 'timeout':
  - **DISPLAY**: "Quick review timed out. Auto-approving task..."
  - **UPDATE** task metadata: `auto_approved: true, approved_by: "timeout", review_mode: "quick_optional"`
  - **PROCEED** to Phase 3 (Implementation)

**ELSE IF** result.action == 'escalate':
  - **DISPLAY**: "Escalating to full review mode..."
  - **UPDATE** review_mode to FULL_REQUIRED
  - **SET** escalated flag: true
  - **FALL THROUGH** to FULL_REQUIRED handling below

**ELSE IF** result.action == 'cancel':
  - **DISPLAY**: "Task cancelled by user"
  - **UPDATE** task metadata: `cancelled: true, cancelled_at: {timestamp}, cancelled_reason: "User cancelled during quick review"`
  - **MOVE** task file from in_progress/ to backlog/
  - **EXIT** task-work command

**ELSE IF** review_mode == FULL_REQUIRED (OR escalated from QUICK_OPTIONAL):

**STEP 1: LOAD PLAN AND DISPLAY ENHANCED CHECKPOINT (TASK-028)**

Load implementation plan from either:
- `docs/state/{task_id}/implementation_plan.md` (Markdown format - TASK-027)
- `docs/state/{task_id}/implementation_plan.json` (JSON format - legacy)

**Display rich visual checkpoint**:

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ðŸŽ¯ PHASE 2.8 - IMPLEMENTATION PLAN CHECKPOINT
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

TASK: TASK-042 - Implement user authentication API

COMPLEXITY: 7/10 (High - Full review required)

ðŸ“ FILES TO CREATE (5 files):
   1. src/auth/login.py           - Login endpoint handler
   2. src/auth/session.py         - Session management
   3. src/auth/validator.py       - Input validation
   4. tests/test_login.py         - Login endpoint tests
   5. tests/test_session.py       - Session management tests

ðŸ“¦ EXTERNAL DEPENDENCIES (3 new packages):
   â€¢ bcrypt - Password hashing
   â€¢ PyJWT - JWT token generation
   â€¢ redis - Session storage

âš ï¸  RISKS IDENTIFIED (2 risks):
   ðŸŸ¡ MEDIUM - External dependency on Redis server
   ðŸ”´ HIGH - Security: Password storage and session tokens

â±ï¸  ESTIMATED EFFORT:
   â€¢ Duration: 8 hours
   â€¢ Lines of Code: ~450 lines
   â€¢ Complexity: High (7/10)

ðŸ—ï¸  IMPLEMENTATION PHASES:
   Phase 1: Models and validation (2h)
   Phase 2: Authentication logic (3h)
   Phase 3: Session management (2h)
   Phase 4: Testing (1h)

ðŸ“Š ARCHITECTURAL REVIEW:
   Overall Score: 85/100 (Approved with recommendations)
   SOLID: 88/100
   DRY: 82/100
   YAGNI: 85/100

OPTIONS:
  [A]pprove  - Proceed with current plan
  [M]odify   - Edit plan before implementation (TASK-029)
  [V]iew     - Show complete plan in pager
  [C]ancel   - Cancel task, return to backlog

Your choice [A/M/V/C]:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**STEP 2: HANDLE USER DECISION**

**IF** user selects [A]pprove:
  - **DISPLAY**: "Plan approved. Proceeding to implementation..."
  - **UPDATE** task metadata: `approved: true, approved_by: "user", approved_at: {timestamp}, review_mode: "full_required"`
  - **IF** escalated: Also update `escalated: true`
  - **PROCEED** to Phase 3 (Implementation)

**ELSE IF** user selects [M]odify (TASK-029 - Interactive Plan Modification):

  **ENTER MODIFICATION LOOP**:

  ```
  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  ðŸ“ PLAN MODIFICATION MODE
  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  Select what to modify:
    1. Files - Add/remove files to create or modify
    2. Dependencies - Add/remove/update external dependencies
    3. Risks - Add/remove/modify risks and mitigations
    4. Effort - Adjust duration or complexity estimates
    5. [U]ndo - Revert to previous version
    6. [D]one - Save changes and return to checkpoint
    7. [C]ancel - Discard changes

  Choice [1-4/U/D/C]:
  ```

  **Modification Options**:

  **1. Modify Files**:
  ```
  Current files to create (5):
    1. src/auth/login.py
    2. src/auth/session.py
    3. src/auth/validator.py
    4. tests/test_login.py
    5. tests/test_session.py

  Actions:
    [A]dd file - Add new file to plan
    [R]emove file - Remove file from plan
    [B]ack - Return to modification menu

  Choice [A/R/B]:
  ```

  **2. Modify Dependencies**:
  ```
  Current dependencies (3):
    1. bcrypt - Password hashing
    2. PyJWT - JWT token generation
    3. redis - Session storage

  Actions:
    [A]dd dependency - Add new package
    [R]emove dependency - Remove package
    [M]odify dependency - Change version or justification
    [B]ack - Return to modification menu

  Choice [A/R/M/B]:
  ```

  **3. Modify Risks**:
  ```
  Current risks (2):
    1. ðŸŸ¡ MEDIUM - External dependency on Redis server
    2. ðŸ”´ HIGH - Security: Password storage and session tokens

  Actions:
    [A]dd risk - Add new risk
    [R]emove risk - Remove risk
    [M]odify risk - Change severity or mitigation
    [B]ack - Return to modification menu

  Choice [A/R/M/B]:
  ```

  **4. Modify Effort**:
  ```
  Current estimates:
    Duration: 8 hours
    Lines of Code: ~450 lines
    Complexity: 7/10 (High)

  Enter new values (or press Enter to keep):
    Duration [8h]: 10h
    LOC [~450]: ~500
    Complexity [7]: 8

  Updated estimates:
    Duration: 10 hours (+25%)
    Lines of Code: ~500 lines (+11%)
    Complexity: 8/10 (High)

  Confirm changes? [y/n]:
  ```

  **Version Management**:
  - Automatically saves plan versions: `implementation_plan_v1.json`, `implementation_plan_v2.json`, etc.
  - Each modification creates timestamped backup
  - [U]ndo option reverts to previous version
  - Plan history tracked in task metadata

  **After Modifications Complete**:
  - Save updated plan to `implementation_plan.md` and `implementation_plan.json`
  - Recalculate complexity score based on new plan
  - Update architectural review if significant changes
  - Return to Phase 2.8 checkpoint display with updated plan
  - Prompt user again: [A]pprove / [M]odify / [V]iew / [C]ancel

  **Modification Metadata** (saved to task frontmatter):
  ```yaml
  plan_modifications:
    - version: 1
      timestamp: "2025-10-19T14:30:00Z"
      changes:
        - category: "dependencies"
          action: "added"
          detail: "Added redis-py package"
        - category: "effort"
          action: "modified"
          detail: "Increased duration from 8h to 10h"
      complexity_before: 7
      complexity_after: 8
    current_version: 1
  ```

**ELSE IF** user selects [V]iew:
  - Display complete plan in pager (less/more command)
  - Return to checkpoint prompt after viewing

**ELSE IF** user selects [C]ancel:
  - **CONFIRM**: "Are you sure you want to cancel? [y/n]:"
  - **IF** confirmed:
    - **DISPLAY**: "Task cancelled by user"
    - **UPDATE** task metadata: `cancelled: true, cancelled_at: {timestamp}, cancelled_reason: "User cancelled during full review"`
    - **MOVE** task file from in_progress/ to backlog/
    - **EXIT** task-work command

#### Phase 2.9: Workflow Routing (NEW - Design-First Workflow Support)

**PURPOSE**: Route to appropriate workflow based on flags from Step 0

**EVALUATE** workflow mode:

```python
# Check which workflow to execute
if design_only:
    # DESIGN-ONLY workflow: Stop here, save design, move to design_approved state
    workflow_mode = "design_only"
elif implement_only:
    # IMPLEMENT-ONLY workflow: Verify prerequisites, skip to Phase 3
    workflow_mode = "implement_only"
else:
    # STANDARD workflow: Continue to Phase 3 as normal
    workflow_mode = "standard"
```

**PATH A: DESIGN-ONLY Workflow** (--design-only flag):

```python
if workflow_mode == "design_only":
    print("\nðŸŽ¨ Design-Only Workflow Complete")
    print("=" * 67)

    # Import plan persistence module
    from installer.core.commands.lib.plan_persistence import save_plan

    # Save implementation plan to disk
    plan_data = {
        "files_to_create": extract_files_to_create(phase_2_output),
        "files_to_modify": extract_files_to_modify(phase_2_output),
        "external_dependencies": extract_dependencies(phase_2_output),
        "estimated_duration": extract_duration(phase_2_output),
        "estimated_loc": extract_loc(phase_2_output),
        "phases": extract_phases(phase_2_output),
        "test_summary": extract_test_summary(phase_2_output),
        "risks": extract_risks(phase_2_output)
    }

    architectural_review = {
        "overall_score": arch_score,
        "status": arch_status,
        "principles": {
            "solid": solid_score,
            "dry": dry_score,
            "yagni": yagni_score
        },
        "recommendations": arch_recommendations
    }

    plan_path = save_plan(task_id, plan_data, architectural_review)
    print(f"âœ… Implementation plan saved: {plan_path}")

    # Update task frontmatter with design metadata
    design_metadata = {
        "status": "approved",
        "approved_at": datetime.now().isoformat(),
        "approved_by": "human",  # or "auto" if auto-approved
        "implementation_plan_version": "v1",
        "architectural_review_score": arch_score,
        "complexity_score": complexity_score.total_score,
        "design_session_id": f"design-{task_id}-{datetime.now().strftime('%Y%m%d%H%M%S')}",
        "design_notes": "Design approved via --design-only workflow"
    }

    # Add design section to task frontmatter
    # ... (update task file with design metadata)

    # Move task to design_approved state
    # ... (move file from current state to tasks/design_approved/)

    # Display success report
    print("\nâœ… Design Phase Complete - " + task_id)
    print()
    print("ðŸŽ¨ Design Approval Summary:")
    print(f"- Architectural Review: {arch_score}/100 ({arch_status})")
    print(f"- Complexity Score: {complexity_score.total_score}/10 ({complexity_score.level})")
    print(f"- Approval Status: APPROVED")
    print(f"- Approved By: {design_metadata['approved_by']}")
    print(f"- Approved At: {design_metadata['approved_at']}")
    print()
    print("ðŸ“‹ Implementation Plan:")
    print(f"- Files to create: {len(plan_data['files_to_create'])}")
    print(f"- External dependencies: {len(plan_data['external_dependencies'])}")
    print(f"- Estimated duration: {plan_data['estimated_duration']}")
    print(f"- Estimated LOC: {plan_data.get('estimated_loc', 'N/A')}")
    print()
    print("ðŸ”„ State Transition:")
    print(f"From: {current_state}")
    print("To: DESIGN_APPROVED")
    print("Reason: Design approved via --design-only workflow")
    print()
    print("ðŸ“‹ Next Steps:")
    print("1. Review the saved implementation plan")
    print("2. Schedule implementation session")
    print(f"3. Run: /task-work {task_id} --implement-only")
    print()
    print("ðŸ’¾ Design artifacts saved to task metadata")

    # EXIT - Do not proceed to Phase 3
    exit(0)
```

**PATH B: IMPLEMENT-ONLY Workflow** (--implement-only flag):

```python
elif workflow_mode == "implement_only":
    print("\nðŸš€ Implement-Only Workflow: Loading Approved Design")
    print("=" * 67)

    # Import phase execution module
    from installer.core.commands.lib.phase_execution import execute_implementation_phases, StateValidationError
    from installer.core.commands.lib.plan_persistence import load_plan, plan_exists

    # Verify task is in design_approved state
    if current_state != "design_approved":
        raise StateValidationError(
            f"âŒ Cannot execute --implement-only workflow\n\n"
            f"Task {task_id} is in '{current_state}' state.\n"
            f"Required state: design_approved\n\n"
            f"To approve design first, run:\n"
            f"  /task-work {task_id} --design-only\n\n"
            f"Or run complete workflow without flags:\n"
            f"  /task-work {task_id}"
        )

    # Verify design metadata exists
    design_metadata = task_context.get("design", {})
    if not design_metadata or design_metadata.get("status") != "approved":
        raise PhaseExecutionError(
            f"âŒ Design metadata missing or invalid for {task_id}\n\n"
            f"Task is in design_approved state, but design metadata is incomplete.\n"
            f"Re-run design phase: /task-work {task_id} --design-only"
        )

    # Verify implementation plan exists
    if not plan_exists(task_id):
        raise PhaseExecutionError(
            f"âŒ Implementation plan not found for {task_id}\n\n"
            f"Design was approved but plan file is missing.\n"
            f"Re-run design phase: /task-work {task_id} --design-only"
        )

    # Load implementation plan
    saved_plan = load_plan(task_id)
    plan_data = saved_plan["plan"]

    # Display implementation start context
    print()
    print(f"TASK: {task_id} - {task_context['title']}")
    print()
    print("APPROVED DESIGN:")
    print(f"  Design approved: {design_metadata.get('approved_at', 'unknown')}")
    print(f"  Approved by: {design_metadata.get('approved_by', 'unknown')}")
    print(f"  Architectural score: {design_metadata.get('architectural_review_score', 'N/A')}/100")
    print(f"  Complexity score: {design_metadata.get('complexity_score', 'N/A')}/10")
    print()
    print("IMPLEMENTATION PLAN:")
    print(f"  Files to create: {len(plan_data.get('files_to_create', []))}")
    print(f"  External dependencies: {len(plan_data.get('external_dependencies', []))}")
    print(f"  Estimated duration: {plan_data.get('estimated_duration', 'N/A')}")
    print(f"  Test strategy: {plan_data.get('test_summary', 'N/A')}")
    print()
    print("Beginning implementation phases (3 â†’ 4 â†’ 4.5 â†’ 5)...")
    print("=" * 67)
    print()

    # Move task from design_approved to in_progress
    # ... (move file from tasks/design_approved/ to tasks/in_progress/)

    # SKIP Phases 1-2.8, jump directly to Phase 3
    # (Phases 1-2.8 results are already in saved_plan)

    # Continue to Phase 3 with loaded plan
    # ... (Phase 3 execution continues below)
```

**PATH C: STANDARD Workflow** (no flags):

```python
else:  # workflow_mode == "standard"
    print("\nðŸ”„ Standard Workflow: Proceeding to Implementation")
    print("Design phases complete, continuing to implementation...\n")

    # No special handling needed - continue to Phase 3 as normal
    # This is the existing behavior (backward compatible)
```

#### Phase 3-BDD: BDD Test Generation (BDD Mode Only)

**IF mode == 'bdd'**:

**DISPLAY INVOCATION MESSAGE**:
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ðŸ¤– INVOKING AGENT: bdd-generator
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Phase: 3-BDD (BDD Test Generation)
Model: Sonnet (Gherkin parsing and test mapping require reasoning)
Mode: BDD (Scenario-driven development)
Specialization:
  - Gherkin scenario parsing (Feature/Scenario/Given/When/Then)
  - BDD framework-specific step definitions
  - Test code generation from scenarios
  - {bdd_framework} integration

Starting agent execution...
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**INVOKE** Task tool:
```
subagent_type: "bdd-generator"
description: "Generate BDD tests for TASK-XXX from Gherkin scenarios"
prompt: "Generate BDD acceptance tests for TASK-{task_id}.

LOADED SCENARIOS:
{for scenario in task_context['gherkin_scenarios']:}
Scenario ID: {scenario['id']}
File: {scenario['file']}
Content:
{scenario['content']}

{endfor}

BDD FRAMEWORK: {task_context['bdd_framework']}
PROJECT STACK: {detected_stack}

REQUIREMENTS:
1. Parse all Gherkin scenarios (Feature/Scenario/Given/When/Then)
2. Generate step definitions for {task_context['bdd_framework']}
   - Python: pytest-bdd step definitions in tests/step_defs/
   - JavaScript/TypeScript: Cucumber.js step definitions
   - .NET: SpecFlow step definitions with C# bindings
   - Ruby: Cucumber step definitions
3. Create test files that execute scenarios
4. Map Given/When/Then steps to test implementation
5. Generate FAILING tests initially (BDD RED phase)
6. Set up BDD test configuration (if needed)

OUTPUT:
- Step definition files matching {task_context['bdd_framework']} conventions
- Test runner configuration (pytest.ini, cucumber.js config, etc.)
- Feature file integration (if copying to project)
- Test data/fixtures as needed
- Documentation showing scenario â†’ step â†’ code mapping

The implementation in next phase (Phase 3) will make these tests pass."
```

**WAIT** for agent to complete before proceeding.

**DISPLAY COMPLETION MESSAGE**:
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âœ… AGENT COMPLETED: bdd-generator
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Duration: {phase_3_bdd_duration_seconds}s
Step definitions created: {step_def_count}
Scenarios mapped: {len(task_context['gherkin_scenarios'])}
Framework: {task_context['bdd_framework']}
Status: BDD tests generated (RED phase) - ready for implementation

Proceeding to Phase 3...
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**PHASE GATE VALIDATION**:
```python
try:
    validator.validate_phase_completion("3-BDD", "BDD Test Generation")
except ValidationError as e:
    print(str(e))
    move_task_to_blocked(task_id, reason="Phase 3-BDD gate violation - agent not invoked")
    exit(1)
```

**IF validation passes**: Proceed to Phase 3
**IF validation fails**: Task moved to BLOCKED, execution stops

**ELSE** (standard or TDD mode):
Skip Phase 3-BDD, proceed directly to Phase 3

#### Phase 3: Implementation

**DISPLAY INVOCATION MESSAGE**:
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ðŸ¤– INVOKING AGENT: {selected_implementation_agent_from_table}
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Phase: 3 (Implementation)
Model: Haiku (Fast implementation, Sonnet for complexity â‰¥7)
Stack: {detected_stack}
Specialization:
  - Production-quality code generation
  - {stack}-specific patterns and conventions
  - Test-driven development support

Starting agent execution...
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**INVOKE** Task tool:
```
subagent_type: "{selected_implementation_agent_from_table}"
description: "Implement TASK-XXX"
prompt: "Implement TASK-XXX following {stack} best practices and planned architecture.
         Use patterns identified in planning phase.
         Create production-quality code with proper error handling.
         Follow {stack}-specific conventions and patterns.
         {if mode == 'bdd':}
         BDD MODE: Implement code to make BDD test step definitions PASS.
         - Focus on making Given/When/Then scenarios pass
         - Follow scenario requirements precisely
         - Implement step definition logic
         - The BDD tests were generated in Phase 3-BDD
         {endif}
         Prepare codebase for comprehensive testing."
```

**WAIT** for agent to complete before proceeding.

**DISPLAY COMPLETION MESSAGE**:
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âœ… AGENT COMPLETED: {selected_implementation_agent_from_table}
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Duration: {phase_3_duration_seconds}s
Files created/modified: {implementation_file_count}
Lines of code: {loc_added}
Error handling: {error_handling_status}
Status: Implementation complete - ready for testing

Proceeding to Phase 4...
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**PHASE GATE VALIDATION** (NEW - TASK-ENF4):
```python
try:
    validator.validate_phase_completion("3", "Implementation")
except ValidationError as e:
    print(str(e))
    move_task_to_blocked(task_id, reason="Phase 3 gate violation - agent not invoked")
    exit(1)
```

**IF validation passes**: Proceed to Phase 4
**IF validation fails**: Task moved to BLOCKED, execution stops

#### Phase 4: Testing

**CRITICAL**: Refer to test-orchestrator.md for mandatory compilation verification before testing.

**DISPLAY INVOCATION MESSAGE**:
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ðŸ¤– INVOKING AGENT: {selected_testing_agent_from_table}
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Phase: 4 (Testing)
Model: Haiku (Fast test execution)
Stack: {detected_stack}
Specialization:
  - Comprehensive test suite generation
  - Coverage analysis and reporting
  - {stack}-specific testing frameworks

Starting agent execution...
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**INVOKE** Task tool with documentation context:
```
subagent_type: "{selected_testing_agent_from_table}"
description: "Generate and execute tests for TASK-XXX"
prompt: "<AGENT_CONTEXT>
documentation_level: {documentation_level}
complexity_score: {task_context.complexity}
task_id: {task_id}
stack: {stack}
phase: 4
</AGENT_CONTEXT>

Create comprehensive test suite for {task_id} implementation.
Include: unit tests, integration tests, edge cases.
Target: 80%+ line coverage, 75%+ branch coverage.
Use {stack}-specific testing frameworks and patterns.

{if mode == 'bdd':}
BDD MODE: Execute BDD tests generated in Phase 3-BDD
- Run BDD scenarios using {task_context.get('bdd_framework')}
- Test commands:
  * Python: pytest tests/ -v --gherkin-terminal-reporter (pytest-bdd)
  * TypeScript/JS: npm run test:bdd or npx cucumber-js (Cucumber.js)
  * .NET: dotnet test --filter Category=BDD (SpecFlow)
  * Ruby: cucumber features/ (Cucumber)
- BDD tests MUST pass 100% (part of Phase 4.5 enforcement)
- Also run standard unit tests for complete coverage
{endif}

ðŸš¨ MANDATORY COMPILATION CHECK (See test-orchestrator.md):
1. MUST verify code COMPILES/BUILDS successfully BEFORE running tests
2. If compilation fails, report errors immediately with file:line details
3. ONLY proceed to test execution if compilation succeeds with zero errors
4. Use stack-specific build commands (see test-orchestrator.md for details)

EXECUTE the test suite and report detailed results:
- Build/compilation status (MUST be success before tests run)
- Test execution results (passed/failed counts)
- Coverage metrics (line and branch percentages)
- Detailed failure information for any failing tests

DOCUMENTATION BEHAVIOR (documentation_level={documentation_level}):
- minimal: Return test results as structured data (counts, coverage, failures). CONSTRAINT: Generate ONLY 2 files maximum.
- standard: Return results with brief test descriptions. CONSTRAINT: Generate ONLY 2 files maximum.
- comprehensive: Generate detailed test report with rationale for each test (13+ files allowed)

Cross-reference: installer/core/agents/test-orchestrator.md (MANDATORY RULE #1)"
```

**WAIT** for agent to complete before proceeding.

**DISPLAY COMPLETION MESSAGE**:
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âœ… AGENT COMPLETED: {selected_testing_agent_from_table}
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Duration: {phase_4_duration_seconds}s
Tests executed: {test_count}
Line coverage: {line_coverage}%
Branch coverage: {branch_coverage}%
Test status: {test_status}
Status: Test suite ready for verification

Proceeding to Phase 4.5...
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**PHASE GATE VALIDATION** (NEW - TASK-ENF4):
```python
try:
    validator.validate_phase_completion("4", "Testing")
except ValidationError as e:
    print(str(e))
    move_task_to_blocked(task_id, reason="Phase 4 gate violation - agent not invoked")
    exit(1)
```

**IF validation passes**: Proceed to Phase 4.5
**IF validation fails**: Task moved to BLOCKED, execution stops

#### Phase 4.5: Fix Loop (Ensure All Tests Pass)

ðŸš¨ **ABSOLUTE REQUIREMENT - ZERO TOLERANCE FOR TEST FAILURES** ðŸš¨

The task-work command has **ZERO TOLERANCE** for compilation errors or test failures.
This phase MUST NOT complete until:
- Code compiles with ZERO errors (100% build success)
- ALL tests pass (100% test pass rate)
- NO tests are skipped, ignored, or commented out

**Cross-reference**: See test-orchestrator.md for quality gate enforcement details.

**EVALUATE** test results from Phase 4:

```python
compilation_errors = extract_compilation_errors(phase_4_output)
test_failures = extract_test_failures(phase_4_output)
coverage = extract_coverage(phase_4_output)

max_attempts = 3
attempt = 1
```

**WHILE** (compilation_errors > 0 OR test_failures > 0) AND attempt <= max_attempts:

1. **DISPLAY** Failure Report:
   ```
   âš ï¸  TESTS FAILING - Entering Fix Loop (Attempt {attempt}/3)

   Compilation Errors: {count}
   {List of compilation errors with file:line}

   Test Failures: {count}
   {List of failing tests with assertion details}

   Initiating automatic fix cycle...
   ```

2. **INVOKE** Task tool to fix issues:
   ```
   subagent_type: "{selected_implementation_agent_from_table}"
   description: "Fix test failures for TASK-XXX (Attempt {attempt})"
   prompt: "Fix the failing tests for TASK-XXX.

            COMPILATION ERRORS ({count}):
            {list_of_compilation_errors_with_file_line}

            TEST FAILURES ({count}):
            {list_of_test_failures_with_details}

            CRITICAL INSTRUCTIONS:
            1. Fix ALL compilation errors FIRST - code must build
            2. Run the build command to verify compilation succeeds
            3. Fix failing test assertions by correcting the implementation
            4. Ensure code behavior matches test expectations
            5. Do NOT modify tests unless they're provably incorrect
            6. Do NOT skip, comment out, or ignore failing tests
            7. Do NOT mark tests with [Ignore] or skip attributes

            SUCCESS CRITERIA:
            - Zero compilation errors
            - All tests pass (100%)
            - No tests skipped or ignored

            You MUST achieve passing tests before completing."
   ```

3. **WAIT** for fix to complete

4. **RE-INVOKE** Phase 4 Testing:
   ```
   subagent_type: "{selected_testing_agent_from_table}"
   description: "Re-run tests for TASK-XXX after fixes (Attempt {attempt})"
   prompt: "Re-execute the complete test suite for TASK-XXX after fixes.

            VERIFY:
            1. Code compiles/builds successfully (no errors)
            2. All tests execute without errors
            3. All tests pass (no failures)
            4. Coverage meets thresholds (â‰¥80% line, â‰¥75% branch)

            Report detailed results including:
            - Build/compilation status
            - Test pass/fail counts
            - Coverage percentages
            - Any remaining failures"
   ```

5. **WAIT** for test execution to complete

6. **RE-EVALUATE** results:
   ```python
   compilation_errors = extract_compilation_errors(retest_output)
   test_failures = extract_test_failures(retest_output)
   attempt += 1
   ```

7. **IF** compilation_errors == 0 AND test_failures == 0:
   ```
   âœ… All tests passing! Proceeding to code review.
   ```
   **BREAK** out of loop â†’ Proceed to Phase 5

8. **ELSE IF** attempt > max_attempts:
   ```
   âŒ CRITICAL: Unable to achieve passing tests after 3 attempts

   Final Status:
   - Compilation Errors: {count}
   - Test Failures: {count}
   - Coverage: {percentage}%

   Task moved to BLOCKED state with detailed diagnostics.
   Manual intervention required.

   Diagnostics have been saved to task file.
   ```
   **BREAK** out of loop â†’ Move to BLOCKED state

9. **ELSE**:
   **CONTINUE** loop (attempt next fix)

**END WHILE**

**Result of Phase 4.5**:
- âœ… **SUCCESS**: All tests passing â†’ Proceed to Phase 5
- âŒ **BLOCKED**: Max attempts exhausted â†’ Move to BLOCKED state, skip Phase 5

#### Phase 5: Code Review

**ONLY EXECUTE IF Phase 4.5 succeeded (all tests passing)**

**DISPLAY INVOCATION MESSAGE**:
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ðŸ¤– INVOKING AGENT: code-reviewer
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Phase: 5 (Code Review)
Model: Sonnet (Expert code quality assessment)
Stack: {detected_stack}
Specialization:
  - Code quality and best practices verification
  - Test coverage validation
  - Documentation and error handling review

Starting agent execution...
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**INVOKE** Task tool with documentation context:
```
subagent_type: "code-reviewer"
description: "Review TASK-XXX implementation"
prompt: "<AGENT_CONTEXT>
documentation_level: {documentation_level}
complexity_score: {task_context.complexity}
task_id: {task_id}
stack: {stack}
phase: 5
</AGENT_CONTEXT>

Review {task_id} implementation for quality and best practices.
Check: code quality, test coverage, error handling, documentation.
Verify {stack}-specific patterns are correctly applied.
Provide actionable feedback if improvements needed.
Confirm readiness for IN_REVIEW state or identify blockers.

DOCUMENTATION BEHAVIOR (documentation_level={documentation_level}):
- minimal: Return approval status and critical issues only. CONSTRAINT: Generate ONLY 2 files maximum.
- standard: Return review with brief feedback on key areas. CONSTRAINT: Generate ONLY 2 files maximum.
- comprehensive: Generate detailed code review report with recommendations (13+ files allowed)

See installer/core/agents/code-reviewer.md for documentation level specifications."
```

**WAIT** for agent to complete before proceeding.

**DISPLAY COMPLETION MESSAGE**:
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âœ… AGENT COMPLETED: code-reviewer
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Duration: {phase_5_duration_seconds}s
Code quality score: {code_quality_score}/100
Issues found: {issues_found_count}
Recommendations: {recommendations_count}
Status: Code review complete - quality approved

Proceeding to Phase 5.5...
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**PHASE GATE VALIDATION** (NEW - TASK-ENF4):
```python
try:
    validator.validate_phase_completion("5", "Code Review")
except ValidationError as e:
    print(str(e))
    move_task_to_blocked(task_id, reason="Phase 5 gate violation - agent not invoked")
    exit(1)
```

**IF validation passes**: Proceed to Phase 5.5
**IF validation fails**: Task moved to BLOCKED, execution stops

#### Phase 5.5: Plan Audit (Hubbard's Step 6)

**NEW PHASE** - Implements John Hubbard's Step 6 (Audit) from his proven 6-step workflow.

**When to execute:**
- Always execute after Phase 5 (Code Review)
- Only if implementation plan exists (skip for tasks without plans)
- Applies to both --implement-only and standard workflows
- NOT executed in --micro mode

**Objective:**
Verify that actual implementation matches the approved architectural plan. Catch scope creep, validate complexity estimates, and ensure AI followed instructions.

**Research Support:**
- John Hubbard's 6-step workflow: "Audit - check the code against Plan.md"
- ThoughtWorks research: "Agent frequently doesn't follow all instructions" - Birgitta BÃ¶ckeler
- Closes critical gap in AI-Engineer Lite identified in SDD research analysis

**Process:**
1. **Load saved implementation plan** from `docs/state/{task_id}/implementation_plan.md`
2. **Analyze actual implementation:**
   - Scan for created/modified files
   - Count lines of code (LOC)
   - Extract dependencies from package files (requirements.txt, package.json, *.csproj)
   - Calculate implementation duration (if available in metadata)
3. **Compare planned vs actual:**
   - **Files**: List extra files, missing files
   - **Dependencies**: List extra deps, missing deps
   - **LOC**: Calculate % variance
   - **Duration**: Calculate % variance
4. **Generate audit report** with severity (low/medium/high)
5. **Display report and prompt** for human decision

**Severity Calculation:**
- **Low**: <10% variance, 0 extra files, all metrics within acceptable range
- **Medium**: 10-30% variance, 1-2 extra files, or 1-2 extra dependencies
- **High**: >30% variance, 3+ extra files, 3+ extra dependencies, or major deviations

**Human Decision Options:**
- **[A]pprove**: Accept implementation as-is, proceed to IN_REVIEW
  - Updates task metadata with audit results
  - Non-blocking default (allows unattended operation)
- **[R]evise**: Request removal of scope creep, transition to BLOCKED
  - Requires manual intervention to remove extra files/dependencies
  - Task cannot proceed until revised
- **[E]scalate**: Create follow-up task, proceed to IN_REVIEW with warning
  - Acknowledges complexity underestimation
  - Creates tracking task for scope creep investigation
  - Current task completes but flagged for analysis
- **[C]ancel**: Block task completion, transition to BLOCKED
  - Complete rejection of implementation
  - Requires full rework

**Timeout Behavior:**
- 30-second timeout for human response
- **Auto-approves if no input** (non-blocking default)
- Allows unattended operation while preserving human control option
- Audit report saved to `docs/state/{task_id}/plan_audit_report.json`

**Metrics Tracking:**
Audit outcomes are tracked in `docs/state/plan_audit_metrics.json` for:
- **Complexity model improvement**: Use LOC/duration variances to refine estimates
- **Scope creep pattern detection**: Identify common sources of extra files/deps
- **Estimation accuracy refinement**: Create feedback loop for better planning

**Example Output:**
```
======================================================================
PLAN AUDIT - TASK-042
======================================================================

PLANNED IMPLEMENTATION:
  Files: 5 files (245 lines)
  Dependencies: 2 (axios, bcrypt)
  Duration: 4 hours

ACTUAL IMPLEMENTATION:
  Files: 7 files (380 lines)
  Dependencies: 3 (axios, bcrypt, lodash)
  Duration: 6 hours

DISCREPANCIES:
  ðŸ”´ 2 extra file(s) not in plan
      - src/utils/helpers.ts
      - src/utils/validators.ts

  ðŸŸ¡ 1 extra dependenc(ies) not in plan
      - lodash

  ðŸ”´ LOC variance: +55.1% (245 â†’ 380 lines)

  ðŸŸ¡ Duration variance: +50.0% (4.0h â†’ 6.0h)

SEVERITY: ðŸ”´ HIGH

RECOMMENDATIONS:
  1. Review extra files for scope creep: src/utils/helpers.ts, src/utils/validators.ts
  2. Justify extra dependencies: lodash
  3. Understand why LOC exceeded estimate by 55%

OPTIONS:
  [A]pprove - Accept implementation as-is, update plan retroactively
  [R]evise - Request removal of scope creep items
  [E]scalate - Mark as complex, create follow-up task
  [C]ancel - Block task completion

Choice [A]pprove/[R]evise/[E]scalate/[C]ancel (30s timeout = auto-approve): _
```

**Implementation:**
Phase 5.5 is implemented in `installer/core/commands/lib/phase_execution.py`:
- Function: `execute_phase_5_5_plan_audit(task_id, task_context)`
- Core logic: `installer/core/commands/lib/plan_audit.py`
- Metrics tracking: `installer/core/commands/lib/metrics/plan_audit_metrics.py`

**Skip Behavior:**
If no implementation plan exists (e.g., task created before Phase 2.7 was implemented), Phase 5.5 is automatically skipped:
```
âš ï¸  No implementation plan found - skipping audit
```

**Success Criteria:**
- Audit completes in < 5 seconds
- Discrepancies accurately detected
- Human decision properly handled
- Task metadata updated correctly
- Metrics tracked for future improvement

**Error Handling:**
- If plan doesn't exist: Skip audit, proceed to IN_REVIEW
- If audit fails: Log error, default to approve (non-blocking)
- If decision timeout: Auto-approve with warning

**Benefits:**
- âœ… Catches scope creep automatically (saves review time)
- âœ… Validates complexity estimates (improves future planning)
- âœ… Ensures AI follows plan (detects hallucinations)
- âœ… Closes Hubbard's Step 6 gap (100% workflow alignment)
- âœ… Creates feedback loop for estimation improvement

### Step 5: Evaluate Quality Gates (REQUIRED)

**Note**: Phase 4.5 (Fix Loop) already enforces test compilation and passing. This step evaluates final quality metrics.

Based on final results after Phase 4.5, **EVALUATE**:

| Gate | Threshold | Result |
|------|-----------|--------|
| Code compiles | 100% | âœ… or âŒ (Phase 4.5 enforced) |
| All tests passing | 100% | âœ… or âŒ (Phase 4.5 enforced) |
| Line coverage | â‰¥ 80% | âœ… or âŒ |
| Branch coverage | â‰¥ 75% | âœ… or âŒ |
| Test execution time | < 30s | âœ… or âš ï¸ |

### Step 6: Determine Next State (REQUIRED)

ðŸš¨ **CRITICAL ENFORCEMENT LOGIC - NO EXCEPTIONS** ðŸš¨

Based on Phase 4.5 results and quality gates, the following logic MUST be enforced:

**BLOCKING LOGIC** (explicit Python pseudocode to prevent IN_REVIEW with failures):

```python
# ABSOLUTE REQUIREMENT: Task CANNOT move to IN_REVIEW unless ALL conditions met
def determine_next_state(phase_45_results, coverage_results):
    """
    Determines next task state with ZERO TOLERANCE for failures.

    Returns:
        state: "in_review" | "blocked" | "in_progress"
        reason: Human-readable explanation
    """
    compilation_errors = phase_45_results.compilation_errors
    test_failures = phase_45_results.test_failures
    test_pass_rate = phase_45_results.passed / phase_45_results.total
    line_coverage = coverage_results.lines
    branch_coverage = coverage_results.branches

    # GATE 1: Compilation must succeed (MANDATORY)
    if compilation_errors > 0:
        return "blocked", f"BLOCKED: {compilation_errors} compilation errors remain after 3 fix attempts"

    # GATE 2: All tests must pass - NO EXCEPTIONS (MANDATORY)
    if test_failures > 0 or test_pass_rate < 1.0:
        return "blocked", f"BLOCKED: {test_failures} test failures remain (pass rate: {test_pass_rate*100:.1f}%)"

    # GATE 3: Coverage thresholds must be met (MANDATORY)
    if line_coverage < 80:
        # Re-invoke testing agent to add more tests
        return "in_progress", f"Coverage too low ({line_coverage}%), generating additional tests"

    if branch_coverage < 75:
        # Re-invoke testing agent to add more tests
        return "in_progress", f"Branch coverage too low ({branch_coverage}%), generating additional tests"

    # ALL GATES PASSED - ONLY path to IN_REVIEW
    return "in_review", "All quality gates passed: 100% tests passing, coverage thresholds met"
```

**STATE TRANSITION RULES**:

- âœ… **Phase 4.5 SUCCESS (all tests passing) + Coverage â‰¥ thresholds**:
  â†’ Move task to `tasks/in_review/TASK-XXX.md`
  â†’ All quality gates passed, ready for human review
  â†’ **This is the ONLY path to IN_REVIEW state**

- âš ï¸ **Phase 4.5 SUCCESS but coverage below threshold**:
  â†’ Keep task in `tasks/in_progress/TASK-XXX.md`
  â†’ **RE-INVOKE** testing agent to add more tests
  â†’ Do NOT proceed until coverage threshold met
  â†’ Loop back to Phase 4
  â†’ **MUST NOT move to IN_REVIEW**

- âŒ **Phase 4.5 BLOCKED (max fix attempts exhausted with failures)**:
  â†’ Move task to `tasks/blocked/TASK-XXX.md`
  â†’ Include detailed diagnostics in task file:
    - Compilation errors (if any)
    - Test failure details
    - Fix attempts made
    - Recommended next steps
  â†’ Notify that manual intervention required
  â†’ **MUST NOT move to IN_REVIEW**

### Step 6.5: Validate Agent Invocations (CRITICAL - Prevent False Reporting)

ðŸš¨ **MANDATORY CHECKPOINT - DO NOT SKIP** ðŸš¨

**Purpose**: Verify that all required agents were actually invoked via the Task tool before generating completion reports. This prevents false reporting where agents are listed as "used" when they were never called.

**CRITICAL**: This validation MUST run before Step 7 (report generation). If validation fails, task MUST be moved to BLOCKED state and report generation MUST be skipped.

**VALIDATE** using the agent invocation tracker:

```python
from installer.core.commands.lib.agent_invocation_validator import (
    validate_agent_invocations,
    ValidationError
)
from installer.core.commands.lib.task_utils import move_task_to_blocked

try:
    # Validate that all required agents were invoked
    validate_agent_invocations(tracker, workflow_mode)
    print("âœ… Validation Passed: All required agents invoked\n")
except ValidationError as e:
    # Display detailed error message
    print(str(e))
    print("\n" + "=" * 55)
    print("BLOCKING TASK DUE TO PROTOCOL VIOLATION")
    print("=" * 55 + "\n")

    # Move task to BLOCKED state
    move_task_to_blocked(
        task_id,
        reason="Agent invocation protocol violation - required agents not invoked"
    )

    # Exit without generating completion report
    exit(1)
```

**Workflow Mode Phase Counts**:
- `standard`: 5 phases (Planning, Arch Review, Implementation, Testing, Code Review)
- `micro`: 3 phases (Implementation, Testing, Quick Review)
- `design-only`: 3 phases (Planning, Arch Review, Complexity)
- `implement-only`: 3 phases (Implementation, Testing, Code Review)

**Expected Behavior**:

âœ… **VALIDATION PASSES** (all phases completed):
- Displays: "âœ… Validation Passed: All required agents invoked"
- Proceeds to Step 7 (Generate Report)
- Task moves to IN_REVIEW state (if quality gates passed)

âŒ **VALIDATION FAILS** (missing phases):
- Displays detailed error showing:
  - Expected vs actual invocation count
  - List of missing phases with descriptions
  - Full agent invocations log
- Moves task to BLOCKED state with reason
- Exits WITHOUT generating completion report
- Human must review why phases were skipped

**Example Error Output** (missing phases):

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âŒ PROTOCOL VIOLATION: Agent invocation incomplete
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Expected: 5 agent invocations
Actual: 3 completed invocations

Missing phases:
  - Phase 3 (Implementation)
  - Phase 4 (Testing)

Cannot generate completion report until all agents are invoked.
Review the AGENT INVOCATIONS LOG above to see which phases were skipped.

AGENT INVOCATIONS LOG:
âœ… Phase 2 (Planning): python-api-specialist (completed in 45s)
âœ… Phase 2.5B (Arch Review): architectural-reviewer (completed in 30s)
âŒ Phase 3: SKIPPED (Not invoked)
âŒ Phase 4: SKIPPED (Not invoked)
âœ… Phase 5 (Review): code-reviewer (completed in 20s)

TASK WILL BE MOVED TO BLOCKED STATE
Reason: Protocol violation - required agents not invoked
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**IMPORTANT NOTES**:

1. **This is the ONLY checkpoint that prevents false reporting**
   - If this step is skipped, completion reports can be generated even when agents weren't invoked
   - This violates the core principle of agent invocation enforcement

2. **Validation is workflow-aware**
   - Different workflow modes have different expected phase counts
   - Micro workflows skip Phase 2 and 2.5B (only 3 phases)
   - Design-only workflows only run Phases 2, 2.5B, 2.7

3. **BLOCKED state requires human review**
   - When validation fails, task goes to BLOCKED
   - Human must investigate why phases were skipped
   - Common causes: direct implementation without agent invocation, manual testing bypass

4. **No exceptions or overrides**
   - Validation cannot be bypassed
   - If workflow needs different phase count, update `get_expected_phases()` function
   - Do NOT skip this step under any circumstances

### Step 7: Generate Report (REQUIRED)

**OUTPUT** comprehensive report based on outcome:

#### Success Report (All Tests Passing)

```
âœ… Task Work Complete - TASK-XXX

ðŸ” Stack: {detected_stack}
ðŸ¤– Agents Used: {list_of_agents}
â±ï¸  Duration: {total_duration}

ðŸ“Š Test Results:
- Compilation: âœ… Success
- Total Tests: {total_tests}
- Passed: {passed_tests} âœ… (100%)
- Failed: 0
- Skipped: 0
- Coverage: {coverage_percentage}% (line), {branch_percentage}% (branch)

ðŸ”§ Fix Loop Summary:
- Initial test run: {initial_failures} failures
- Fix attempts: {fix_attempts_made}
- Final result: All tests passing âœ…

ðŸ“ˆ Quality Gates:
âœ… Code compiles
âœ… All tests passing (100%)
âœ… Line coverage ({coverage}% â‰¥ 80%)
âœ… Branch coverage ({branch}% â‰¥ 75%)
{performance_status} Test execution time ({time}s)

ðŸ”„ State Transition:
From: IN_PROGRESS
To: IN_REVIEW
Reason: All quality gates passed

ðŸ“‹ Next Steps:
- Human review of implementation
- Merge to main branch if approved
- Deploy to staging environment
```

#### Blocked Report (Tests Still Failing)

```
âŒ Task Work Blocked - TASK-XXX

ðŸ” Stack: {detected_stack}
ðŸ¤– Agents Used: {list_of_agents}
â±ï¸  Duration: {total_duration}

ðŸ“Š Final Test Results:
- Compilation: {compilation_status}
- Total Tests: {total_tests}
- Passed: {passed_tests}
- Failed: {failed_tests} âŒ
- Skipped: {skipped_tests}
- Coverage: {coverage_percentage}%

ðŸ”§ Fix Loop Summary:
- Initial failures: {initial_failures}
- Fix attempts made: 3/3 (max reached)
- Remaining issues: {remaining_issues}

âŒ Remaining Compilation Errors ({count}):
{list_of_compilation_errors}

âŒ Remaining Test Failures ({count}):
{list_of_test_failures}

ðŸ“ˆ Quality Gates:
{compile_status} Code compiles
âŒ Tests passing ({failed} failures)
{coverage_status} Coverage thresholds

ðŸ”„ State Transition:
From: IN_PROGRESS
To: BLOCKED
Reason: Unable to achieve passing tests after 3 fix attempts

ðŸ“‹ Required Actions:
1. Review compilation errors (if any) and fix manually
2. Review test failure details and diagnose root cause
3. Check for missing dependencies or configuration issues
4. Verify test specifications are correct
5. Consider if architectural changes are needed
6. Re-run /task-work once issues are manually resolved

ðŸ’¡ Recommendations:
{specific_recommendations_based_on_error_patterns}
```

### Step 8: Commit State Files to Git (REQUIRED for Conductor Support)

**CRITICAL**: After completing all phases and generating the report, commit all state files to git. This ensures that state is preserved across git worktrees (used by Conductor.build for parallel development).

**EXECUTE** the following Python code:

```python
from installer.core.commands.lib.git_state_helper import commit_state_files

# Commit all state files for this task
# This includes:
# - docs/state/{task_id}/implementation_plan.md
# - docs/state/{task_id}/audit_report.json (if Phase 5.5 executed)
# - Any other state files created during workflow

try:
    commit_state_files(
        task_id="{task_id}",
        message=f"Save implementation state for {task_id} (workflow complete)"
    )
    print("âœ… State files committed to git")
except Exception as e:
    # Don't fail workflow if git commit fails
    # (may not be in a git repo, or git may not be available)
    print(f"âš ï¸  Warning: Could not commit state files: {e}")
    print("   (This is non-critical - workflow can continue)")
```

**Why this is needed:**

- **Conductor.build** uses git worktrees for parallel development
- Each worktree has its own working directory but shares the same git repository
- State files in `docs/state/` MUST be committed to be visible across all worktrees
- Without this step, state loss occurs when switching between worktrees

**When to skip:**

- Only skip if not in a git repository (e.g., running in a sandboxed environment)
- Error handling ensures workflow continues even if git commit fails

**What gets committed:**

- All files in `docs/state/{task_id}/` directory
- Commit message includes task ID for traceability
- Does NOT push to remote (that's a separate operation)

---

## ðŸ“š ADDITIONAL CONTEXT (Reference Only - Execute Above First)

### Development Modes

The command supports multiple development modes via `--mode` flag:

#### Standard Mode (Default)
```bash
/task-work TASK-XXX
```
- Implementation and tests together
- Fastest approach for straightforward features
- All 5 phases execute in sequence

#### TDD Mode
```bash
/task-work TASK-XXX --mode=tdd
```
- RED: Testing agent generates failing tests first
- GREEN: Implementation agent writes minimal code to pass
- REFACTOR: Implementation agent improves code quality
- Best for complex business logic

#### BDD Mode (Requires RequireKit)

```bash
/task-work TASK-XXX --mode=bdd
```

**Purpose**: Behavior-Driven Development workflow for formal agentic systems

**Prerequisites**:
- RequireKit installed (checks `~/.agentecflow/require-kit.marker.json` or legacy `require-kit.marker`)
- Task has `bdd_scenarios: [BDD-001, BDD-002]` in frontmatter

**Use for**:
- âœ… Agentic orchestration systems (LangGraph, state machines)
- âœ… Safety-critical workflows (quality gates, approval checkpoints)
- âœ… Complex behavior requirements (multi-agent coordination)
- âœ… Formal specifications (compliance, audit, traceability)
- âŒ NOT for general CRUD features or simple implementations

**Workflow**:
1. **Phase 1**: Validates RequireKit installation via marker file
2. **Phase 1**: Loads Gherkin scenarios from task frontmatter
3. **Phase 2**: Includes scenarios in planning context
4. **Phase 3**: Routes to RequireKit's bdd-generator agent
5. **Phase 3**: Generates step definitions for detected framework
6. **Phase 3**: Implements code to pass scenarios
7. **Phase 4**: Runs BDD tests (pytest-bdd, SpecFlow, Cucumber.js, etc.)
8. **Phase 4.5**: Fix loop for failing BDD tests (max 3 attempts)
9. **Phase 5**: Standard code review

**Error Handling**:

If RequireKit not installed:
```bash
/task-work TASK-042 --mode=bdd

ERROR: BDD mode requires RequireKit installation

  RequireKit provides EARS â†’ Gherkin â†’ Implementation workflow for
  formal behavior specifications.

  Repository:
    https://github.com/requirekit/require-kit

  Installation:
    cd ~/Projects/require-kit
    ./installer/scripts/install.sh

  Verification:
    ls ~/.agentecflow/require-kit.marker.json  # Should exist (or require-kit.marker for legacy)

  Alternative modes:
    /task-work TASK-042 --mode=tdd      # Test-first development
    /task-work TASK-042 --mode=standard # Default workflow

  BDD mode is designed for agentic systems, not general features.
  See: docs/guides/bdd-workflow-for-agentic-systems.md
```

If bdd_scenarios not linked:
```bash
/task-work TASK-042 --mode=bdd

ERROR: BDD mode requires linked Gherkin scenarios

  Task frontmatter must include bdd_scenarios field:

    ---
    id: TASK-042
    title: Implement complexity routing
    bdd_scenarios: [BDD-ORCH-001, BDD-ORCH-002]  â† Add this
    ---

  Generate scenarios in RequireKit:
    cd ~/Projects/require-kit
    /formalize-ears REQ-XXX
    /generate-bdd REQ-XXX

  Or use alternative modes:
    /task-work TASK-042 --mode=tdd
    /task-work TASK-042 --mode=standard
```

**BDD Framework Detection**:
- Python project â†’ pytest-bdd
- .NET project â†’ SpecFlow
- TypeScript/JavaScript â†’ Cucumber.js
- Ruby â†’ Cucumber

**See**: [BDD Workflow Guide](../../docs/guides/bdd-workflow-for-agentic-systems.md)

### Agent Discovery System

**Dynamic Metadata-Based Matching**

Agents are selected dynamically based on metadata matching, NOT from static tables. The system:

1. Analyzes task context (stack, phase, keywords from description)
2. Scans all agent sources for metadata matches
3. Returns best match based on:
   - Stack compatibility (python, react, dotnet, etc.)
   - Phase alignment (implementation, review, testing, orchestration, debugging)
   - Keyword relevance (capabilities match task requirements)

**No Hardcoded Mappings**: Agent selection is intelligent and extensible - adding new agents automatically makes them discoverable.

#### Discovery Sources and Precedence

Agents are discovered from 4 sources in priority order:

1. **Local** (`.claude/agents/`) - Highest priority
   - Template agents copied during initialization
   - Project-specific customizations
   - **Always takes precedence** over global agents with same name

2. **User** (`~/.agentecflow/agents/`)
   - Personal agent customizations
   - Available across all projects
   - Overrides global agents with same name

3. **Global** (`installer/core/agents/`)
   - Built-in GuardKit agents
   - Shared across all users
   - Overridden by local/user agents

4. **Template** (`installer/core/templates/*/agents/`) - Lowest priority
   - Template-provided agents (before initialization)
   - Only used if agent not found in higher-priority sources
   - Replaced by local agents after `guardkit init`

**Precedence Rule**: Local > User > Global > Template

#### Template Override Behavior

When you run `guardkit init <template>`:
- Template agents copied to `.claude/agents/` (local)
- Local agents now **override** global agents with same name
- Enables template customization without modifying global agents

**Example**:
```bash
# Before initialization
/task-work TASK-001  # Uses global python-api-specialist

# After initialization
guardkit init fastapi-python
# Template's python-api-specialist copied to .claude/agents/

/task-work TASK-002  # Now uses LOCAL python-api-specialist ðŸ“ (not global ðŸŒ)
```

#### Metadata Requirements for Discovery

For an agent to be discoverable, it must have:

| Field | Type | Required | Purpose |
|-------|------|----------|---------|
| `stack` | array | âœ… Yes | Technology stack(s) the agent supports |
| `phase` | string | âœ… Yes | Workflow phase (implementation, review, testing, etc.) |
| `capabilities` | array | âœ… Yes | Specific skills and domains |
| `keywords` | array | âœ… Yes | Searchable terms for matching |

**Agents without metadata**: Skipped during discovery (graceful degradation).

#### Fallback Behavior

If no specialist agent is found:
- System falls back to `task-manager` (cross-stack orchestrator)
- Task-manager handles the task generically
- User notified about fallback in invocation log

#### Agent Source Indicators

During task execution, the invocation log shows which agent was selected and its source:

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
AGENT INVOCATIONS LOG
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âœ… Phase 2 (Planning): python-api-specialist ðŸ“ (source: local, completed in 45s)
âœ… Phase 2.5B (Arch Review): architectural-reviewer ðŸŒ (source: global, completed in 30s)
âœ… Phase 3 (Implementation): python-api-specialist ðŸ“ (source: local, completed in 120s)
âœ… Phase 4 (Testing): task-manager ðŸŒ (source: global, completed in 60s)
âœ… Phase 5 (Review): code-reviewer ðŸŒ (source: global, completed in 25s)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Source Icons**:
- ðŸ“ **Local** - Agent from `.claude/agents/` (template or custom)
- ðŸ‘¤ **User** - Agent from `~/.agentecflow/agents/` (personal)
- ðŸŒ **Global** - Agent from `installer/core/agents/` (built-in)
- ðŸ“¦ **Template** - Agent from `installer/core/templates/*/agents/` (before init)

**Why Source Matters**:
- **Local agents override global** - Verify template customizations working
- **Precedence debugging** - Understand which agent was selected when duplicates exist
- **Troubleshooting** - If wrong agent selected, check source and metadata

#### Agent Discovery Examples

##### Example 1: Python API Implementation

**Task Context**:
- Stack: Python
- Files: `*.py`
- Keywords: "FastAPI endpoint", "async", "Pydantic schema"

**Discovery Process**:
1. Detect stack: `python` (from file extensions)
2. Detect phase: `implementation` (Phase 3)
3. Extract keywords: `api`, `async`, `pydantic`
4. Scan agents:
   - Local `.claude/agents/python-api-specialist.md` âœ… Match
   - Metadata: `stack: [python, fastapi]`, `phase: implementation`, `keywords: [api, async, pydantic]`
5. **Selected**: `python-api-specialist ðŸ“ (source: local)`

##### Example 2: React State Management

**Task Context**:
- Stack: React
- Files: `*.tsx`
- Keywords: "hooks", "state", "TanStack Query"

**Discovery Process**:
1. Detect stack: `react` (from file extensions)
2. Detect phase: `implementation` (Phase 3)
3. Extract keywords: `hooks`, `state`, `query`
4. Scan agents:
   - Local `.claude/agents/react-state-specialist.md` âœ… Match
   - Metadata: `stack: [react, typescript]`, `phase: implementation`, `keywords: [hooks, state, query]`
5. **Selected**: `react-state-specialist ðŸ“ (source: local)`

##### Example 3: Architectural Review (Cross-Stack)

**Task Context**:
- Stack: Any
- Phase: Architectural Review (Phase 2.5B)

**Discovery Process**:
1. Phase: `review` (architectural)
2. Scan agents:
   - Global `installer/core/agents/architectural-reviewer.md` âœ… Match
   - Metadata: `stack: [cross-stack]`, `phase: review`, `keywords: [solid, dry, yagni, architecture]`
3. **Selected**: `architectural-reviewer ðŸŒ (source: global)`

##### Example 4: Fallback to Task-Manager

**Task Context**:
- Stack: Go
- Files: `*.go`
- Keywords: "service", "handler"

**Discovery Process**:
1. Detect stack: `go` (from file extensions)
2. Detect phase: `implementation` (Phase 3)
3. Scan agents:
   - No agents with `stack: [go]` found
4. **Fallback**: `task-manager ðŸŒ (source: global)`
5. **Note**: User notified that task-manager used (no Go specialist available)

**Fix**: Create go-specialist agent with appropriate metadata, or use cross-stack task-manager.

#### Troubleshooting Agent Discovery

**Issue**: "Expected agent not selected"

**Debug Steps**:
1. Check agent has required metadata (stack, phase, capabilities, keywords)
2. Verify stack matches task technology (check file extensions)
3. Check agent source in invocation log (ðŸ“ local, ðŸ‘¤ user, ðŸŒ global, ðŸ“¦ template)
4. Verify precedence isn't causing unexpected override

**Issue**: "Task-manager used instead of specialist"

**Causes**:
- No specialist agent exists for the stack
- Agent metadata doesn't match task context
- Agent missing required metadata fields

**Fix**: Run `/agent-enhance` to add/validate discovery metadata.

### Stack-Specific Agent Details (Examples Only)

**Note**: The agents listed below are examples from global/template sources. Actual agent selection is **dynamic** based on metadata matching (see [Agent Discovery System](#agent-discovery-system)). Local agents override these examples after template initialization.

**To view available agents**: Run `/agent-list` or check:
- Local: `.claude/agents/`
- User: `~/.agentecflow/agents/`
- Global: `installer/core/agents/`

#### MAUI Stack Agents
- **maui-usecase-specialist**: UseCase pattern with Either monad
- **maui-viewmodel-specialist**: MVVM with RelayCommand
- **dotnet-testing-specialist**: xUnit with FluentAssertions

#### React Stack Agents
- **react-state-specialist**: Hooks, context, state management
- **react-testing-specialist**: React Testing Library, Vitest

#### Python Stack Agents
- **python-api-specialist**: FastAPI, Pydantic, async patterns
- **python-testing-specialist**: pytest, pytest-asyncio, fixtures

#### Python MCP Stack Agents
- **python-mcp-specialist**: MCP server architecture, tool/resource registration, LangGraph integration
- **python-testing-specialist**: pytest, pytest-asyncio, MCP client testing

#### TypeScript API Stack Agents
- **nestjs-api-specialist**: NestJS, dependency injection, decorators
- **typescript-domain-specialist**: Domain modeling, Result patterns
- **nodejs-testing-specialist**: Jest, Supertest, integration tests

#### .NET Microservice Stack Agents
- **dotnet-api-specialist**: FastEndpoints, REPR pattern, middleware
- **dotnet-domain-specialist**: DDD, Either monad, domain events
- **dotnet-testing-specialist**: xUnit, WebApplicationFactory, Testcontainers

### Usage Examples

#### Basic Usage
```bash
# Automatic stack detection and full workflow
/task-work TASK-042
```

#### With Options
```bash
# TDD mode with higher coverage threshold
/task-work TASK-042 --mode=tdd --coverage-threshold=90

# Fix only mode (for blocked tasks)
/task-work TASK-042 --fix-only

# With progress sync to epic/feature
/task-work TASK-042 --sync-progress

# Include full epic/feature context
/task-work TASK-042 --with-context
```

### Technology Detection Priority

1. **Primary**: Read `project.template` from `.claude/settings.json`
2. **Fallback**: Auto-detect from project files:
   - `*.csproj` with `Microsoft.Maui` â†’ maui
   - `*.csproj` with `FastEndpoints` â†’ dotnet-microservice
   - `package.json` with `react` â†’ react
   - `package.json` with `@nestjs` â†’ typescript-api
   - `requirements.txt` or `pyproject.toml` with `mcp` dependency â†’ python-mcp
   - `requirements.txt` or `pyproject.toml` â†’ python
3. **Default**: Use generic agents (software-architect, task-manager, test-verifier)

### Quality Gate Details

#### Tests Passing (Required)
- All test cases must pass
- No skipped tests allowed
- No test errors or warnings

#### Line Coverage (Required â‰¥ 80%)
- Percentage of code lines executed during tests
- Excludes generated code, interfaces
- Calculated by stack-specific coverage tool

#### Branch Coverage (Required â‰¥ 75%)
- Percentage of conditional branches tested
- Both true and false paths must be covered
- Critical for logic-heavy code

#### Performance (Warning if > 30s)
- Total test suite execution time
- Warning only, doesn't block
- Suggests optimization if exceeded

### Error Handling

#### Scenario: Task Not Found
```
âŒ Error: Task TASK-XXX not found
Location checked: tasks/in_progress/TASK-XXX.md
Action: Verify task ID or check task state (backlog/blocked/completed)
```

#### Scenario: Tests Failing
```
âŒ Task TASK-XXX - Tests Failed

Failed Tests:
1. test_feature_validation (line 45)
   Expected: ValidationError
   Actual: None

Action: Review implementation and run:
/task-work TASK-XXX --fix-only
```

#### Scenario: Low Coverage
```
âš ï¸  Task TASK-XXX - Coverage Below Threshold

Current: 72%
Required: 80%

Uncovered:
- feature_service.py lines 45-52 (error handling)
- feature_service.py lines 78-85 (edge case)

Action: Testing agent will generate additional tests automatically
```

### Advanced Options

```bash
# Dry run (show plan without executing)
/task-work TASK-XXX --dry-run

# Watch mode (continuous testing)
/task-work TASK-XXX --watch

# Parallel test execution
/task-work TASK-XXX --parallel

# Skip specific phase
/task-work TASK-XXX --skip-review

# Force specific agent
/task-work TASK-XXX --implementation-agent=custom-specialist
```

### Integration with External Tools

When task metadata includes external tool references:

```yaml
# In task frontmatter
external_tools:
  jira: PROJ-123
  linear: PROJECT-456
  github: #789
```

After successful completion, automatically sync:
- Update Jira sub-task status to "In Review"
- Update Linear issue progress to 100%
- Update GitHub issue with test results

### File Locations

```
tasks/
â”œâ”€â”€ backlog/         # New tasks (BACKLOG state)
â”œâ”€â”€ in_progress/     # Active work (IN_PROGRESS state)
â”œâ”€â”€ in_review/       # Passed quality gates (IN_REVIEW state)
â”œâ”€â”€ blocked/         # Failed quality gates (BLOCKED state)
â””â”€â”€ completed/       # Finished tasks (COMPLETED state)
```

### Success Metrics

After running `/task-work`:
- âœ… All agents invoked automatically
- âœ… No manual intervention required
- âœ… Quality gates enforced consistently
- âœ… State transitions handled automatically
- âœ… Comprehensive report generated

### Troubleshooting

**Problem**: Agents not invoked
- **Cause**: Command reading stopped before execution protocol
- **Fix**: Ensure execution protocol is first content Claude sees

**Problem**: Wrong agents selected
- **Cause**: Stack detection failed or incorrect settings
- **Fix**: Verify `.claude/settings.json` has correct `project.template`

**Problem**: Task tool not found
- **Cause**: Claude Code version doesn't support Task tool
- **Fix**: Update Claude Code to latest version

**Problem**: Agent not found
- **Cause**: Stack-specific agent doesn't exist
- **Fix**: System falls back to default agents automatically

### Best Practices

1. **Always start with `/task-work`** - Don't manually implement
2. **Trust the agents** - They're specialized for their domains
3. **Review quality gate failures** - They indicate real issues
4. **Use appropriate mode** - TDD for logic, BDD for features
5. **Keep tasks focused** - One feature per task works best

### Migration from Previous System

If you previously used separate commands:
- âŒ `/task-implement` â†’ Use `/task-work`
- âŒ `/task-test` â†’ Use `/task-work`
- âŒ Manual quality checks â†’ Automatic in `/task-work`

### Command Philosophy

**"Implementation and testing are inseparable"**

This command embodies quality-first development by:
- Combining implementation with test creation
- Automatically running tests after implementation
- Enforcing quality gates before state transitions
- Supporting multiple development methodologies

Part of the streamlined 3-command workflow:
1. `/task-create` - Define the work
2. `/task-work` - Build and verify (THIS COMMAND)
3. `/task-complete` - Ship it

---

## âš ï¸ CRITICAL REMINDER

**DO NOT ATTEMPT TO IMPLEMENT THE TASK YOURSELF**

This command requires **Task tool invocations for each phase**. Your role is to:
1. âœ… Detect the stack
2. âœ… Select the correct agents
3. âœ… Invoke Task tool for each phase
4. âœ… Aggregate results and generate report

**DO NOT**:
- âŒ Write implementation code directly
- âŒ Write test code directly
- âŒ Skip agent invocations
- âŒ Attempt to do all phases yourself

The agents are specialized and will produce better results than doing it yourself.
