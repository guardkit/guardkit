# TASK-AGENT-BOUND: Add Explicit Boundaries to All Agents

**Task ID**: TASK-AGENT-BOUND-20251121-120100
**Priority**: CRITICAL (P0)
**Status**: BACKLOG
**Created**: 2025-11-21
**Tags**: agents, boundaries, always-ask-never, github-best-practices

---

## Overview

Add explicit "ALWAYS/ASK/NEVER" boundary sections to all 15 global agents, following GitHub's best practice framework from their 2,500+ repository analysis.

**Current Problem**:
- No agent explicitly defines what it will always do, what requires human approval, or what it will never do
- Boundaries are implicit in quality gates and thresholds
- Developers confused about agent authority and autonomy
- Leads to misuse, errors, and unnecessary escalations

**GitHub's Recommendation**:
> Three-tier boundary system: **ALWAYS do**, **ASK first**, **NEVER do**

**Impact**: Clarifies agent autonomy, reduces errors, prevents misuse

---

## Acceptance Criteria

### AC1: Boundary Section Structure

Every agent MUST have a "Boundaries" section with this exact structure:

```markdown
## Boundaries

### ALWAYS (Non-Negotiable)
These actions happen automatically without asking:
- [Action 1 with specific threshold/metric]
- [Action 2 with specific threshold/metric]
- [Action 3 with specific threshold/metric]
- [Action 4 with specific threshold/metric]
- [Action 5 with specific threshold/metric]
- [Optional: Actions 6-7]

**Examples**:
[1-2 concrete scenarios showing ALWAYS actions in practice]

### NEVER (Will Be Rejected)
These actions are prohibited:
- [Prohibition 1 with explanation]
- [Prohibition 2 with explanation]
- [Prohibition 3 with explanation]
- [Prohibition 4 with explanation]
- [Prohibition 5 with explanation]
- [Optional: Prohibitions 6-7]

**Examples**:
[1-2 concrete scenarios showing what will be rejected]

### ASK (Escalate to Human)
These situations require human decision:
- [Situation 1 with trigger conditions]
- [Situation 2 with trigger conditions]
- [Situation 3 with trigger conditions]
- [Optional: Situations 4-5]

**Decision Process**:
[Brief explanation of how escalation works]

**Examples**:
[1-2 concrete scenarios showing when to escalate]
```

**Acceptance**:
- [ ] All 15 global agents have Boundaries section
- [ ] Each agent has 5-7 ALWAYS rules
- [ ] Each agent has 5-7 NEVER rules
- [ ] Each agent has 3-5 ASK situations
- [ ] Each tier has concrete examples
- [ ] Boundaries placed at lines 151-200 (after Quick Start)

---

### AC2: Consistency Across Agents

**Boundary rules MUST be**:
- **Specific**: Include thresholds, metrics, conditions (not vague "ensure quality")
- **Testable**: Can verify if rule was followed (not subjective "write good code")
- **Actionable**: Clear what to do (not philosophical "consider trade-offs")

**Example - GOOD**:
```markdown
### ALWAYS
- Block task if compilation fails (100% requirement, zero tolerance)
- Run spec drift detection before quality review (required first step)
- Enforce ≥80% line coverage, ≥75% branch coverage (quality gate thresholds)
```

**Example - BAD**:
```markdown
### ALWAYS
- Ensure code quality
- Check for issues
- Follow best practices
```

**Acceptance**:
- [ ] No vague rules ("ensure", "consider", "try to")
- [ ] All rules include specific thresholds/metrics
- [ ] All rules are testable (can verify compliance)

---

### AC3: Alignment with Quality Gates

**Boundary rules MUST align with existing quality gates**:

**For code-reviewer**:
- ALWAYS: Compilation 100%, Tests 100% pass, Coverage ≥80%/75%
- NEVER: Approve with failing tests, Skip security checks
- ASK: Complexity >7 + critical, Coverage 70-79%

**For architectural-reviewer**:
- ALWAYS: Score all SOLID principles, DRY, YAGNI
- NEVER: Auto-approve <60/100, Skip SOLID analysis
- ASK: Score 60-79, Complexity >7 + high risk

**For test-verifier**:
- ALWAYS: Build before test, 100% test pass rate, Block on persistent failures
- NEVER: Skip build verification, Lower quality standards
- ASK: Coverage 70-79%, Framework configuration changes

**Acceptance**:
- [ ] Boundary rules match documented quality gates
- [ ] Thresholds consistent with phase documentation
- [ ] No contradictions between boundaries and quality gates

---

## Files to Update (15 Global Agents)

### Priority 1 (Week 1, 5 hours)
1. `installer/global/agents/code-reviewer.md`
2. `installer/global/agents/architectural-reviewer.md`
3. `installer/global/agents/task-manager.md`
4. `installer/global/agents/test-verifier.md`
5. `installer/global/agents/test-orchestrator.md`

### Priority 2 (Week 2, 5 hours)
6. `installer/global/agents/build-validator.md`
7. `installer/global/agents/complexity-evaluator.md`
8. `installer/global/agents/pattern-advisor.md`
9. `installer/global/agents/debugging-specialist.md`
10. `installer/global/agents/software-architect.md`

### Priority 3 (Week 3, 5 hours)
11. `installer/global/agents/qa-tester.md`
12. `installer/global/agents/devops-specialist.md`
13. `installer/global/agents/security-specialist.md`
14. `installer/global/agents/database-specialist.md`
15. `installer/global/agents/agent-content-enhancer.md`

---

## Implementation Examples

### Example 1: code-reviewer.md

**Current State** (Implicit Boundaries):
- Lines 439-444: "MUST RUN FIRST - Block review if fails"
- Lines 543-553: Approval checklist (implicit ALWAYS)
- Lines 490-540: Phase 2.6 checkpoint (implicit ASK)
- No explicit NEVER rules

**New Boundaries Section** (Insert at line 250):

```markdown
## Boundaries

### ALWAYS (Non-Negotiable)
These actions happen automatically without asking:
- **Build verification first**: Run compilation check before any code review (block if fails)
- **Spec drift detection**: Execute compliance check before quality review (≥90% required)
- **Test execution verification**: Confirm 100% test pass rate (auto-fix up to 3 attempts)
- **Coverage enforcement**: Enforce ≥80% line coverage, ≥75% branch coverage thresholds
- **Security scan**: Block critical vulnerabilities (SQL injection, XSS, hardcoded secrets)
- **Requirements compliance**: Verify all EARS requirements implemented (100% match)
- **Metadata updates**: Update task metadata after state transitions

**Examples**:
- **Build fails** → Task automatically moved to BLOCKED, implementation requested fixes
- **Coverage 75%** → Review flags insufficient coverage, requests additional tests

### NEVER (Will Be Rejected)
These actions are prohibited:
- **Approve without tests**: Never approve code that doesn't compile or has failing tests
- **Skip compliance check**: Never skip spec drift detection (scope creep prevention)
- **Lower coverage standards**: Never approve <80% line / <75% branch coverage without escalation
- **Ignore security issues**: Never approve code with critical security vulnerabilities (OWASP Top 10)
- **Proceed with scope creep**: Never approve unspecified features without human decision
- **Modify production code**: Never edit implementation code (only flag issues)
- **Auto-merge**: Never commit/merge code automatically (human approves final merge)

**Examples**:
- **Tests fail after 3 auto-fix attempts** → Task BLOCKED, human must resolve
- **Coverage 72%** → Review rejects, requests more tests (below 80% threshold)

### ASK (Escalate to Human)
These situations require human decision:
- **High complexity + critical**: Complexity score >7 AND (security sensitive OR performance critical) → Trigger Phase 2.6 checkpoint
- **Borderline coverage**: Test coverage 70-79% (below threshold but close) → Ask: Accept trade-off or require more tests?
- **Scope creep detected**: Unspecified features implemented (spec drift <90%) → Ask: Remove/Approve/Ignore?
- **Architectural violations**: Issues that should've been caught in Phase 2.5 → Escalate to process improvement
- **Quality score 5-6.9/10**: Borderline quality (below 7/10 target) → Ask: Block or proceed with recommendations?

**Decision Process**:
Agent pauses workflow, presents findings with options, human makes final call, workflow continues based on decision.

**Examples**:
- **Complexity 8 + security-sensitive** → Checkpoint triggered, human reviews plan before implementation
- **Coverage 78%** → Agent asks: "Close to threshold. Accept or add more tests?" Human decides.
```

---

### Example 2: architectural-reviewer.md

**Current State** (Implicit Boundaries):
- Lines 485-488: Scoring thresholds (implicit ALWAYS/NEVER)
- Lines 492-500: Checkpoint triggers (implicit ASK)
- No explicit boundary framework

**New Boundaries Section** (Insert at line 160):

```markdown
## Boundaries

### ALWAYS (Non-Negotiable)
These actions happen automatically without asking:
- **SOLID evaluation**: Score all 5 SOLID principles (0-10 each) before any approval
- **DRY assessment**: Evaluate code duplication (0-25 points) - reject God objects, code clones
- **YAGNI enforcement**: Assess over-engineering risk (0-25 points) - reject speculative features
- **Scoring methodology**: Calculate total score (0-100) using documented rubric
- **Auto-approve ≥80**: Approve designs scoring ≥80/100 automatically (proceed to Phase 3)
- **Auto-reject <60**: Reject designs scoring <60/100 automatically (return to planning)
- **Design-only review**: Review architecture BEFORE implementation (Phase 2.5, not Phase 5)

**Examples**:
- **Design scores 85/100** → Auto-approved, implementation proceeds to Phase 3
- **Design scores 45/100** → Auto-rejected, critical SOLID violations, return to Phase 2

### NEVER (Will Be Rejected)
These actions are prohibited:
- **Review after implementation**: Never review code in Phase 5 (that's code-reviewer's job)
- **Skip SOLID analysis**: Never approve without evaluating all 5 principles
- **Ignore anti-patterns**: Never approve God classes, shotgun surgery, primitive obsession
- **Auto-approve <60/100**: Never approve critical architectural violations
- **Skip YAGNI evaluation**: Never approve designs without over-engineering assessment
- **Subjective scoring**: Never use vague criteria (must use documented 0-10 rubric)
- **Modify implementation**: Never edit code (only review planned architecture)

**Examples**:
- **Request to review implemented code** → Rejected, redirect to code-reviewer
- **Score 55/100 with SRP violations** → Rejected, requires redesign

### ASK (Escalate to Human)
These situations require human decision:
- **Borderline score 60-79**: Approaching approval threshold → Ask: Proceed with recommendations or redesign?
- **Complexity >7 + high risk**: Complex design with security/performance/breaking changes → Trigger Phase 2.6 checkpoint
- **Impact high + experience low**: High-risk changes by junior developers → Trigger Phase 2.6 checkpoint
- **Multiple anti-patterns**: 3+ anti-patterns detected across categories → Escalate for pattern review
- **Trade-off decisions**: SOLID conflict (e.g., SRP vs DRY) → Ask human to prioritize principle

**Decision Process**:
Agent presents architecture scores, identified issues, and recommendations. Human reviews context (team, timeline, risk) and decides: Approve/Revise/Reject.

**Examples**:
- **Score 72/100 (borderline)** → Agent asks: "7 minor issues found. Approve with fixes or redesign?" Human decides.
- **Complexity 9 + breaking changes** → Checkpoint triggered, human reviews before implementation starts
```

---

### Example 3: test-verifier.md

**New Boundaries Section**:

```markdown
## Boundaries

### ALWAYS (Non-Negotiable)
These actions happen automatically without asking:
- **Build before test (MANDATORY RULE #0)**: Run build verification before executing tests (zero tolerance)
- **Empty project detection (MANDATORY RULE #1)**: Skip tests if project has no source files
- **100% test pass rate**: Enforce zero test failures (no "flaky test" exceptions)
- **Auto-fix loop**: Attempt automatic fixes up to 3 times for failing tests
- **Block on persistent failures**: Move task to BLOCKED after 3 failed fix attempts
- **Coverage calculation**: Calculate line and branch coverage after all tests pass
- **Stack-specific commands**: Use correct test framework for technology (pytest, npm test, dotnet test)

**Examples**:
- **Build fails** → Tests skipped, task BLOCKED (must compile first)
- **Tests fail, auto-fix succeeds** → Tests re-run, task proceeds if pass

### NEVER (Will Be Rejected)
These actions are prohibited:
- **Run tests without building**: Never execute tests before compilation verified
- **Accept test failures**: Never approve <100% test pass rate
- **Skip empty project check**: Never run tests on projects with no source files
- **Lower quality standards**: Never reduce coverage thresholds without human approval
- **Ignore flaky tests**: Never mark failures as "acceptable" (fix or remove test)
- **Modify test framework config**: Never change testing infrastructure without human approval
- **Proceed after 3 failed fixes**: Never continue after auto-fix exhausted (escalate)

**Examples**:
- **Tests fail, 3 auto-fix attempts exhausted** → Task BLOCKED, human must resolve
- **Request to lower coverage threshold** → Rejected, escalate to human decision

### ASK (Escalate to Human)
These situations require human decision:
- **Coverage 70-79%**: Below 80% threshold but test quality high → Ask: Accept trade-off or require more tests?
- **Framework configuration change**: Modify test runner settings → Escalate for review
- **Performance test failures**: Perf tests fail but functional tests pass → Ask: Investigate or skip?
- **Flaky test pattern**: Same test fails intermittently → Ask: Fix, disable, or investigate?

**Decision Process**:
Agent pauses after 3 auto-fix attempts, presents failure analysis, human decides: Fix manually, adjust threshold, or simplify test.

**Examples**:
- **Coverage 77% after 3 fix attempts** → Agent asks: "3% short of 80% target. Accept or add tests?" Human decides.
```

---

## Implementation Process

### Step 1: Extract Implicit Boundaries (1 hour per agent)

For each agent:

1. **Find ALWAYS rules** (current implicit behavior):
   - Quality gate thresholds (e.g., "≥80% coverage")
   - Mandatory first steps (e.g., "build before test")
   - Zero-tolerance rules (e.g., "100% test pass rate")
   - Automatic actions (e.g., "auto-fix loop")

2. **Find NEVER rules** (current prohibitions):
   - Out-of-scope actions (e.g., "don't review implemented code")
   - Standard violations (e.g., "don't lower coverage")
   - Security risks (e.g., "don't approve with SQL injection")

3. **Find ASK rules** (current checkpoint triggers):
   - Borderline metrics (e.g., "coverage 70-79%")
   - High complexity conditions (e.g., "complexity >7 + critical")
   - Human decision points (e.g., "scope creep remediation")

### Step 2: Write Boundaries Section (30 min per agent)

1. Create Boundaries section header
2. List 5-7 ALWAYS rules with thresholds
3. List 5-7 NEVER rules with explanations
4. List 3-5 ASK rules with trigger conditions
5. Add 1-2 concrete examples per tier
6. Add "Decision Process" explanation for ASK tier

### Step 3: Validate Consistency (15 min per agent)

Check:
- [ ] Boundaries align with quality gates
- [ ] Thresholds match phase documentation
- [ ] No contradictions with workflow
- [ ] Rules are specific (not vague)
- [ ] Rules are testable (verifiable)
- [ ] Examples are concrete (not abstract)

### Step 4: Integration Testing (15 min per agent)

Test:
- [ ] Run agent with boundary-violating input → Expect rejection
- [ ] Run agent with borderline metric → Expect escalation
- [ ] Run agent with passing metric → Expect auto-approval
- [ ] Verify boundaries displayed in agent output

---

## Testing

### Automated Validation

```bash
# 1. Check all agents have Boundaries section
for agent in installer/global/agents/*.md; do
    if grep -q "## Boundaries" "$agent"; then
        echo "✅ $agent: Has Boundaries"
    else
        echo "❌ $agent: Missing Boundaries"
    fi
done

# 2. Check all agents have ALWAYS section
for agent in installer/global/agents/*.md; do
    if grep -q "### ALWAYS" "$agent"; then
        echo "✅ $agent: Has ALWAYS"
    else
        echo "❌ $agent: Missing ALWAYS"
    fi
done

# 3. Check all agents have NEVER section
for agent in installer/global/agents/*.md; do
    if grep -q "### NEVER" "$agent"; then
        echo "✅ $agent: Has NEVER"
    else
        echo "❌ $agent: Missing NEVER"
    fi
done

# 4. Check all agents have ASK section
for agent in installer/global/agents/*.md; do
    if grep -q "### ASK" "$agent"; then
        echo "✅ $agent: Has ASK"
    else
        echo "❌ $agent: Missing ASK"
    fi
done

# 5. Validate boundaries have examples
for agent in installer/global/agents/*.md; do
    if grep -A 20 "### ALWAYS" "$agent" | grep -q "**Examples**:"; then
        echo "✅ $agent: ALWAYS has examples"
    else
        echo "⚠️ $agent: ALWAYS missing examples"
    fi
done
```

### Manual Testing

**Test Scenario 1: Boundary Violation**
1. Run code-reviewer on code with 75% coverage (below 80% threshold)
2. **Expected**: Task blocked, message references NEVER rule
3. **Verify**: Developer understands why rejected

**Test Scenario 2: Borderline Metric**
1. Run architectural-reviewer on design scoring 72/100 (60-79 range)
2. **Expected**: Human checkpoint triggered, ASK rule cited
3. **Verify**: Developer presented with decision options

**Test Scenario 3: Auto-Approval**
1. Run test-verifier on passing tests with 85% coverage
2. **Expected**: Auto-approved, ALWAYS rule confirmed in output
3. **Verify**: Developer sees what threshold was met

---

## Success Metrics

### Before (Current State)
- **Boundary clarity**: 6-7/10 (implicit in text)
- **Developer confusion**: "Can the agent do X?" questions common
- **Misuse incidents**: Agents used outside intended scope
- **Escalation clarity**: When to ask for human decision unclear

### After (Target)
- **Boundary clarity**: 9-10/10 (explicit ALWAYS/NEVER/ASK) ✅
- **Developer confusion**: <5% ask "Can agent do X?" (boundaries are explicit)
- **Misuse incidents**: <2 per quarter (boundaries prevent misuse)
- **Escalation clarity**: 100% understand when human decision needed

### Developer Feedback (Survey)
- "How clear are the agent's boundaries?" → Target: ≥8/10
- "Do you know what the agent will always/never do?" → Target: ≥90% yes
- "Is it clear when you need to make a decision?" → Target: ≥90% yes

---

## Related Documents

- **Source**: GitHub's AGENTS.md best practices (2,500+ repos)
- **Gap Analysis**: `docs/analysis/github-agent-best-practices-comparison.md`
- **Quality Gates**: Phase documentation (2.5, 4.5, 5.5)

---

## Workflow

```bash
# 1. Create task
/task-work TASK-AGENT-BOUND

# 2. Week 1: Priority 1 agents (5 hours)
# Extract implicit boundaries, write explicit sections

# 3. Week 2: Priority 2 agents (5 hours)
# Repeat for supporting agents

# 4. Week 3: Priority 3 agents (5 hours)
# Repeat for specialized agents

# 5. Validation (2 hours)
# Run automated checks, manual testing

# 6. Complete task
/task-complete TASK-AGENT-BOUND
```

---

## Notes

**Why ALWAYS/ASK/NEVER Framework Works**:
- **ALWAYS**: Builds trust (developer knows what's automatic)
- **ASK**: Empowers human decision-making (agent doesn't overstep)
- **NEVER**: Prevents misuse (clear guardrails)

**Developer Impact**:
- **Before**: "Can the agent approve code with 75% coverage?" (Developer guesses)
- **After**: "NEVER: Approve <80% line coverage" (Developer knows immediately)

**Reduction in Questions**:
- Boundaries answer 80% of "Can agent do X?" questions upfront
- Saves 5-10 min per clarification × 10 clarifications/week = 1 hour/week saved

---

**Created**: 2025-11-21
**Estimated Effort**: 15 hours (3 weeks, 5 hours/week)
**Expected Improvement**: Boundary clarity 6/10 → 9/10
**Status**: BACKLOG
**Ready for Implementation**: YES
